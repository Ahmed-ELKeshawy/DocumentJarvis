{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020D82C775E0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\urllib3\\connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\urllib3\\connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 495\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\urllib3\\connection.py:398\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\http\\client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\http\\client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m \n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\http\\client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\urllib3\\connection.py:236\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\urllib3\\connection.py:211\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x0000020D82C775E0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020D82C775E0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     20\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour input text here\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 21\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mget_ollama_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(embedding)\n",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m, in \u001b[0;36mget_ollama_embedding\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      7\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour-ollama-model-name\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Replace with your Ollama model name\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [text]\n\u001b[0;32m     10\u001b[0m }\n\u001b[1;32m---> 11\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Assuming the response structure\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020D82C775E0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_ollama_embedding(text: str):\n",
    "    url = \"http://127.0.0.1:11434/v1/embeddings\"  # Change to Ollama's API endpoint\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": \"your-ollama-model-name\",  # Replace with your Ollama model name\n",
    "        \"inputs\": [text]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['embeddings'][0]  # Assuming the response structure\n",
    "    else:\n",
    "        print(\"Error:\", response.text)\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "text = \"Your input text here\"\n",
    "embedding = get_ollama_embedding(text)\n",
    "\n",
    "print(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path):\n",
    "    return extract_text(file_path)\n",
    "\n",
    "# Test with a sample PDF file\n",
    "text_data = load_pdf(\"testerpdf.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amedo\\AppData\\Local\\Temp\\ipykernel_22228\\1296967160.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma()\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document and MetaData Added\n",
      "Added 47 chunks for file:  home/path1/paper1/book1\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "def split_text(text,filename,path ,chunk_size=1000, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    documentObject = splitter.split_documents([Document(page_content=text,metadata={\"Path\": path,\"fileName\": filename})])\n",
    "    print(\"Document and MetaData Added\")\n",
    "    return documentObject\n",
    "\n",
    "\n",
    "# Add chunks to Chroma with metadata\n",
    "documenData = split_text(text_data,'page1Researchpaper','home/path1/paper1/book1')\n",
    "print(f\"Added {len(documenData)} chunks for file: \",documenData[0].metadata['Path'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43madd_document_to_chromadb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpaper1-testiing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhome/page1/book1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m, in \u001b[0;36madd_document_to_chromadb\u001b[1;34m(text, filename, path, chunk_size, chunk_overlap)\u001b[0m\n\u001b[0;32m     19\u001b[0m     metadatas\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m: filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: path})\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Add chunks to Chroma with metadata\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks for file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:268\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39madd_texts \u001b[38;5;241m!=\u001b[39m VectorStore\u001b[38;5;241m.\u001b[39madd_texts:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m--> 268\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;66;03m# If there's at least one valid ID, we'll assume that IDs\u001b[39;00m\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;66;03m# should be used.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:268\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39madd_texts \u001b[38;5;241m!=\u001b[39m VectorStore\u001b[38;5;241m.\u001b[39madd_texts:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m--> 268\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;66;03m# If there's at least one valid ID, we'll assume that IDs\u001b[39;00m\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;66;03m# should be used.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "add_document_to_chromadb(text_data, 'paper1-testiing', 'home/page1/book1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='Pattern Recognition Letters 115 (2018) 101–106 \\n\\nContents lists available at ScienceDirect \\n\\nPattern  Recognition  Letters \\n\\njournal homepage: www.elsevier.com/locate/patrec \\n\\nHybrid  deep  neural  networks  for  face  emotion  recognition \\n\\nNeha Jain a , ∗, Shishir Kumar a , Amit Kumar a , Pourya Shamsolmoali b , c , \\nMasoumeh Zareapoor b \\na \\nDepartment of Computer Science & Engineering , Jaypee University of Engineering and Technology , Guna, India \\nb \\nInstitute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai 200240, China \\nc \\nAdvanced Scientiﬁc Computing Division, Euro-Mediterranean Centre on Climate Change (CMCC Foundation), Lecce, Italy \\n\\na r t i c l e \\n\\ni n f o \\n\\na b s t r a c t \\n\\nArticle history: \\nAvailable online 9 April 2018 \\n\\nKeywords: \\nEmotion recognition \\nDeep learning \\nRecurrent neural networks \\nConvolutional Neural Networks \\nHybrid CNN-RNN'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='Deep Neural Networks (DNNs) outperform traditional models in numerous optical recognition missions \\ncontaining Facial Expression Recognition (FER) which is an imperative process in next-generation Human- \\nMachine Interaction (HMI) for clinical practice and behavioral description. Existing FER methods do not \\nhave high accuracy and are not suﬃcient practical in real-time applications. This work proposes a Hybrid \\nConvolution-Recurrent Neural Network method for FER in Images. The proposed network architecture \\nconsists of Convolution layers followed by Recurrent Neural Network (RNN) which the combined model \\nextracts the relations within facial images and by using the recurrent network the temporal dependencies \\nwhich exist in the images can be considered during the classiﬁcation. The proposed hybrid model is eval- \\nuated based on two public datasets and Promising experimental results have been obtained as compared \\nto the state-of-the-art methods.'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='© 2018 Elsevier B.V. All rights reserved. \\n\\n1. Introduction'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='Facial and emotional expressions are the most signiﬁcant non- \\nverbal ways for expressing internal emotions and intentions.“Facial \\nAction Coding system (FACS) is a useful structure that classiﬁes the \\nhuman facial actions by their advent on the face using Action Units \\n(AU). An AU is one of 46 minor elements of visible facial motion or \\nits relatedform changes.Facial expressions have worldwide mean- \\ning, and these emotions have been accepted for tens and even hun- \\ndreds of years and it was the main reason for us to select facial ex- \\npressions for the research”. These days, interest in emotion recog- \\nnition (ER) has skyrocketed, while it stayed as single main diﬃcul- \\nties in the area of human-computer interaction. The cornerstone \\nof  the  most  relevant  research  is  to  build  a  reliable  conversation \\nand communication among human and computer (machine). The \\nimportance of ER methods can be achieved by either “make hu-'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='importance of ER methods can be achieved by either “make hu- \\nmans to understand computer/machine accurately and conversely”. \\nFacial  Expression  Recognition  (FER)  is  a  challenging  task  in  ma- \\nchine learning with a wide-ranging of applications in healthcare, \\nhuman-computer interaction, and gaming. Emotion recognition is \\nchallenging due to several input modalities, have a signiﬁcant role \\nin understanding it. “The mission of recognizing of the emotions is \\nmostly  diﬃcult  dueto  two  main  reasons:  1)  There  is  not  largely'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='∗ Corresponding author. \\n\\nE-mail address: neha.juet@gmail.com (N. Jain). \\n\\nhttps://doi.org/10.1016/j.patrec.2018.04.010 \\n0167-8655/© 2018 Elsevier B.V. All rights reserved.'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='available  database  of  training  images  and  2)  classifying  emotion \\ncould  not  be  simplebased  on  whether  the  input  image  is  static \\nor aevolution frame into a facial expression. The ﬁnaldiﬃculty is \\nmostly for the real-time detection while facial expressions differ- \\nenthusiastically”.  Ekman  et  al.  [1]  counted  six  expressions  (sur- \\nprise, fear, happiness, anger, disgust, and sadness) as main emo- \\ntional expressions that are common among human beings. Mostly \\nthe big overlap between the emotion classes makes the classiﬁca- \\ntion task very diﬃcult. This paper proposed a deep learning tech- \\nnique in the context of emotional recognition, in order to classify \\nemotion labels from the images. Too many methods and research \\nhas been developed in this regards, however, most current works \\nare appeared focusing on hand-engineered features [2,3] . Now a \\nday’s due to quantity and variety of datasets, deep learning is be-'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='day’s due to quantity and variety of datasets, deep learning is be- \\ncoming  as  mainstream  techniques  in  all  computer  visions  tasks \\n[4,5] . Conventional convolutional neural systems have a notewor- \\nthy constraint that they simply handle spatial image. The essential \\ncommitment of this work is to display the spatio-worldly develop- \\nment of outward appearances of a man in the Images utilizing a \\n“Recurrent Neural Network (RNN) which embedded with a Convo- \\nlutional Neural Network (CNN) in a form of CNN–RNN design”. We \\nadditionally introduce a neural system based element level com- \\nbination procedure to join diverse modalities for the last emotion \\nforecast. The pioneering works in emotion recognition based deep \\nlearning [6,7] has achieved the state-of-the-art. The cornerstone of \\nthese proposed models [6,8] is an average-based aggregation for \\nvisual  features.  A  little  distinguish  from  current  works,  we  pro-'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='102 \\n\\nN. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106 \\n\\nposed an RNN to classify the facial emotion. The proposed model \\nexplores feature level fusion strategy and proves the moderate im- \\nprovement by this model. The other parts of the paper are orga- \\nnized as: next section delivers the related work in what we follow. \\nSection 3 presents the proposed network. The results and exper- \\niments are included in Section 4 . At the end, we have concluded \\nour observation in Section 5 . \\n\\n2. Related work'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='“Generally, research works in this area have been focused on \\nidentifying human emotion in the base of video footage or based \\non audiovisual records (mixing speech recognition and video tech- \\nniques).  Several  papers  pursue  to  identify  and  match  faces  [20] , \\nnevertheless  most  works  did  not  use  deep  learning  to  extract \\nemotions from images”. Customarily, calculations for mechanized \\noutward appearance acknowledgment comprises of three primary \\nmodules,  viz.  enlistment,  highlight  extraction,  and  arrangement. \\nPoint by point study of various approaches in every one of these \\nmeans can be found in [9] . “Customary calculations for full of feel- \\ning  registering  from  faces  utilize  designed  highlights  for  exam- \\nple,  Histogram  of  Oriented  Gradients  [11] ,  Local  Binary  Patterns \\n[10] and facial historic points [12] ”. Since the greater parts of these \\nhighlights are hand-created for their particular use of acknowledg-'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='highlights are hand-created for their particular use of acknowledg- \\nment, so the generalization in the particular situation is necessary, \\nsuch as, high variability in lighting, subjects ethnicity, visual de- \\ntermination, and so on. Interestingly, the powerful methodologies \\nfor accomplishing a great acknowledgment for series of marking \\nerrand are alluded to separate the transient relations of edges in \\nan arrangement. Separating these transient relations have been ex- \\namined utilizing customary techniques before. Cases of these en- \\ndeavors  are  Concealed  Markov  Models  [13,14,47,48]  “(which  join \\nthe  data  and  then  apply  division  on  recordings),  Spatio  Tempo- \\nral  Shrouded  Markov  Models  by  combing  S-HMM  and  T-HMM \\n[15] , Dynamic Bayesian Networks” [16] is related to multi-tactile \\ndata combination paradigm, Bayesian transient models to catch the \\ndynamic  outward  appearance  progress,  and  Conditional  Irregular'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='dynamic  outward  appearance  progress,  and  Conditional  Irregular \\nFields (CRFs) [17,18] and their augmentations. Recently, \"Convolu- \\ntional  Neural  Networks\"  (CNN)  has  turned  into  the  most  main- \\nstream approach in the deep learning techniques. AlexNet [19] de- \\npends on the conventional layered engineering which comprises of \\na few convolution layers, max-pooling layers and Rectiﬁed Linear \\nUnits (ReLUs). Szegedy et al. [20] presented GoogLeNet which is \\nmade out of numerous \"Beginning” layers. Commencement applies \\na few convolutions on the include outline distinctive scales. Molla- \\nhosseini et al. [21] have utilized the Inception layer for the under- \\ntaking of outward appearance acknowledgment and accomplished \\nbest in class comes about. Following the accomplishment of Incep- \\ntion layers, a few varieties of them have been proposed [22] . “RNNs \\nrecently have greatly succeeded in handling sequential data such'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='recently have greatly succeeded in handling sequential data such \\nas speech recognition [23] , natural language processing [24,25] , ac- \\ntion recognition [26] , and so on. Then RNN is additionally has been \\nimproved  to  treat  the  images  [27]  by  scanning  the  parts  of  im- \\nages into sequences in certain directions. Due to the capability of \\nrecollecting information about the past inputs, RNN has the abil- \\nity to learn relative dependencies with images, which is advanta- \\ngeous in comparison with CNN. The reason is CNN may fail to learn \\nthe overall dependencies because of the locality of convolution and \\npooling  layers.  Therefore,  RNN  is  generally  combined  with  CNN \\nin order to achieve better achievement in image processing tasks \\nsuch as image recognition [28] and segmentation [29] ”. Conven- \\ntional Recurrent Neural Networks  (RNNs) can learn ﬂeeting pro- \\ngression by mapping input successions to a grouping of concealed'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='gression by mapping input successions to a grouping of concealed \\nstates, and furthermore mapping the covered up states to yields. \\nZhang et al. [30] “proposed a novel deep learning framework called \\nas a spatial-temporal recurrent neural network (STRNN) to unify'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='the learning of two different signal sources into a spatial-temporal \\ndependency  model”.  Khorrami  et  al.  in  [31,45,46] ,  developed  a \\nmethod which used the CNN and RNN in order to perform emo- \\ntion recognition on video data. Chernykh et al. [32] and Fan et al. \\n[33] proposed CNN + RNN models for the video and speech recog- \\nnition. In spite of the fact that RNNs have demonstrated promis- \\ning execution on different assignments, it is diﬃcult for them to \\nlearn  long  haul  successions.  This  is  mostly  the  result  of  vanish- \\ning/detonating slopes issue [34] that can be understood by having \\na memory for recalling and overlooking the past states. “Xie and \\nHu [42] presented a new CNNmodel that used convolutional mod- \\nules. tominimize redundancy of same features learned, considers \\ncommunal information among ﬁlters of the same layer, and offers \\nthe top set of features for the next layer.A distinguishedapplication'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='the top set of features for the next layer.A distinguishedapplication \\nof a CNN to real-time detection of emotions from facial expressions \\nis by Oullet [43] . Theymade a game, while a CNN was applied to \\na video stream to grab the subject’s facial expressions, performing \\nas a controller for the game. This work established the possibil- \\nity of executing a CNN in real-time by means of a running-average \\nof the perceived emotions from the input stream, decreasing the \\nspecial effects of variation and noise. A latest development by Levi \\net al. [44] illustrated important upgrading in facial emotion recog- \\nnition using a CNN. They listed two main drawbacks: 1) the small \\namount of available data for training deep CNNs and 2) appearance \\ndissimilarity generally affected by dissimilarities in illumination”.'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='Distinct from other work including video and RNN strategies, \\n[35] ,  in  this  paper  we  don’t  utilize  LSTMs.  However,  we  utilize \\nIRNNs  [36]  that  is  made  out  of  amended  straight  units  (ReLUs) \\nwhat’s more; utilize a unique introduction system in view of scaled \\nvarieties  of  the  character  grid.  These  components  of  IRNNs  are \\ngone  for  giving  a  substantially  less  diﬃcult  system  to  managing \\nwith the vanishing and detonating inclination issue thought about \\nto  the  more  perplexing  LSTM  system.  Late  work  has  contrasted \\nIRNNs and LSTMs and found that IRNNs can yield equivalent out- \\ncomes in a few errands, including issues which include long haul \\nconditions” [36] . We give point by point details of the CNN and \\nthe RNN structure the in next Section. Moreover, we concatenated \\nthe CNN highlights to a permanent distance feature vectors and \\nfurthermore, trained on SVM. \\n\\n3. Proposed model'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='3. Proposed model \\n\\nThe opposition dataset has only a single emotion label for each \\npicture and do not have relation to each casing. This presents a \\ngreat deal of commotion if the picture labels are utilized as focuses \\non preparing a CNN on the singular image. Our visual highlights \\nare in this way given by a CNN prepared on a mix of two extra \\nemotion datasets of static pictures. In addition, utilizing extra in- \\nformation covers a bigger assortment of age and character rather \\nthan  the  test  information  where  a  similar  performing  artist/on- \\nscreen character might show up in numerous clips. For the CNN \\ntraining we used two large emotion datasets, MMI Facial Expres- \\nsion Database (TFD) [37] it consists more than 2900 images of 75 \\nsubjects and “Japanese Female Facial Expression (JAFFE) Database \\n[38] containing 213 pictures, which have seven basic expressions: \\nangry, sad, surprise, happy, disgust, fear, and neutral”.'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='For  the  preprocessing,  we  represent  ﬂuctuating  lighting  con- \\nditions  (speciﬁcally,  crosswise  over  datasets)  we  connected  his- \\ntogram evening out. We utilized the adjusted appearances gave by \\nthe coordinators to remove highlights from the CNN. “The arrange- \\nment  includes  a  joined  facial  key  point’s  location  and  following \\nmethodology clariﬁed in [39] . Extraordinary confront location, as \\nwell as arrangement procedures, have been utilized for MMI Facial \\nExpression and the JAFFE Datasets”. Keeping in mind the end goal \\nto be ready to use the extra datasets, we re-adjusted all datasets \\nto JAFFE utilizing the accompanying method: \\n\\n\\x0cN. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106 \\n\\n103 \\n\\nEq. (3) illustrates the Adam update and its combination with \\n\\nthe momentum update. \\nm t = β1 m t−1 + (1 − β1 ) ∇ X t−1 \\nv t = β2 v t−1 + (1 − β2 ) ∇ ( X t−1 ) 2 \\nX t = X t−1 − a m t √ \\n\\nv t + ε \\n\\n(3)'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='β1, β2 ∈ [0,1] and ɛ are hyperparameters, m t is the momentum \\nvector with t iteration, v t  is the velocity vector, and the learning \\nrate of α. Adam is the actual update algorithm due to information \\nusage for the primary and the secondary moments of the gradient. \\nThe CNN is used primarily for feature extraction and we have \\njust  utilized  the  extra  dataset  for  the  training.  Accordingly,  we \\nhunt  down  a  model  that  have  better  communalize  to  different \\ndatasets. Profound models are known to learn portrayals to have \\nbetter communalize to different datasets. By the way, it has been \\nfound out that the deep structure rapidly over-ﬁtted, and commu- \\nnalize severely to the test dataset. This could be because of the \\ngenerally little measure of marked information accessible for the \\nemotion detection tasks. Consequently, “we build different connec- \\ntions between 6 layers which seem to have decent tended to the'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='tions between 6 layers which seem to have decent tended to the \\nover-ﬁtting issues. At the end, we expanded the ﬁlter size from 3 \\nto 5 and the numbers of channels are 8-16-32-64-128-256. For the \\nexperimentations data augmentation has been used” ((horizontal, \\nvertical and rotation ﬂipping with 0.25 probability), and dropout is \\nused (with the rate of 0.5).'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='RNNs are a form of neural network that converts the order of \\ninputs into a series of outputs. In separately time step t , an un- \\nknown parameter h t  is calculated according to the unknown pa- \\nrameter at time t − 1 and the input x t at time t \\nh t = σ ( W in x t + W rec h t−1 ) \\n\\n(4) \\n\\nWhile “W \\n\\nin  is the weight of input matrix, W rec  is the matrix \\nof recurrent and σ is the hidden activation function. Respectively \\ntime step similarly calculates the outputs, relying on the existing \\nhidden state”: \\ny t = f ( W out h t ) \\n\\n(5)'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='While W out is the result weighted parameters and f is the ac- \\ntivation function of the output. An instance of an RNN in which \\nmerely the last phase creates the output which illustrated in Fig. 3 . \\n“An  RNN  model  has  been  used,  that  previously  discussed  by \\nusing  rectiﬁed  linear  units  (ReLUs)  and  recurrent  matrix,  which \\nis  adjusted  with  scaled  deviations  of  the  distinctiveness  matrix”\\n[42] . The distinctiveness initialization model conﬁrms good gradi- \\nent movement at the commencement of training and it consents to \\ntrain it on moderately extensive orders. The RNN ha been trained \\nto categorize the images by inserting the extracted features of each \\nimage from the CNN serially network and ﬁnally using the Soft- \\nmax for the prediction. In the implementation the Gradient clip- \\nping rated to 1.0 and a batch size set to 32. We tested the model \\nby using several layers of the CNN as input features and picked the'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='by using several layers of the CNN as input features and picked the \\noutput of every third convolutional layer right after max pooling, \\nas this achieved the highest result on validation data.'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='Fig. 1.  Sample of JAFFE dataset for ﬁve type of emotion (Ang, Sad, Fea, Hap, Sur). \\n\\n1. We distinguished ﬁve facial key focuses for all pictures in the \\nJAFFE and MMI preparing set utilizing the convolutional neural \\nsystem course strategy in [40] . \\n\\n2. for every dataset, the mean shape have been processed by av- \\n\\neraging the directions of main focuses. \\n\\n3. The datasets have been mapped by utilizing a closeness change \\namong  the  mean  shapes.  By  processing  one  change  for  each \\ndataset the nose, eyes, and mouth is generally in a similar area \\nholding a slight measure of variety. We included an uproarious \\nfringe for MMI and JAFFE-faces as appearances were edited all \\nthe more ﬁrmly contrasted with JAFFE. \\n\\n4. JAFFE-faces  approval  test  sets  were  mapped  to  utilize  the \\n\\nchange construed on the preparation set.'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='change construed on the preparation set. \\n\\nAdditionally, dataset standardization has been performed by us- \\ning the standard deviation and mean picture from the consolidated \\nJAFFE and MMI (JAFFE + MMI). Fig. 1 represents the samples face \\nemotion data. For the implementation and the evaluation of the \\nproposed model the 70% of each dataset used for training and the \\nrest 30% for testing. \\n\\n3.1. Convolution neural network architecture \\n\\nEmotion  Recognition  data  comes  in  various  sizes  and  resolu- \\ntions, so we try to propose a model which can handle any type \\nof input. In our approach, “we considered a class of networks with \\n6 convolutional layers plus 2 fully connected layers”, each with a \\nReLu activation function, and dropout for training.'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='Plus 2 fully connected layers”, each with a ReLu activation func- \\ntion, and dropout for training. Furthermore, we performed regular- \\nization for each weight matrix W that limits the size of the weights \\nat individual layer by adding a term to the loss equal to some ﬁxed \\nhyperparameter. We explain these in Eq. (1) , where x be the output \\nof a particular neuron in the network and p the dropout possibility. \\n\\n(cid:2) \\n\\nReLu (x ) = max (0 , x ) \\nDropout (x, p) = \\nReg(w ) = λ|| w || 2 \\n2 \\n\\nx, with prob . p \\nx, with prob . 1 − p \\n\\n(1) \\n\\n3.2. Regression CNN \\n\\nCombinations of two deep learning initializer algorithms have \\nbeen  used  to  perform  parameter  updates  based  on  the  gradi- \\nent of the loss function called as Momentum and Adam [41,42] . \\nEq. (2) describes this update, where X t is parameter matrix at iter- \\nation t. v t is the velocity vector at iteration t, and α is the rate of \\nlearning. \\nv t = γ v t−1 − a ∇ X t−1 \\nX t = X t−1 + v t \\n\\n(2)'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='(2) \\n\\nFirstly  we  used  a  single  CNN  model  to  train  the  datasets.  At \\neach time trained a single image, the corresponding image passed \\nthrough the CNN model, the details of the model shown in Fig. 2 . \\nTwo fully-connected layers with 200 hidden units for the ap- \\nproximation of the valence label have been used. For the cost func- \\ntion the mean squared error has been used. For the network train- \\ning stochastic gradient descent while the batch size sets to 32 and \\nthe weight decay sets to 1E-4. Moreover, the learning rate at the \\nbeginning sets to 5e-3 which decrees by 0.01 every 20 epochs. \\n\\n\\x0c104 \\n\\nN. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='104 \\n\\nN. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106 \\n\\nFig. 2.  CNN Architecture, the network contains six convolutional layers containing \\n8, 16, 32, 64, 128, and 256 ﬁlters; each of size 5 × 5 and 3 × 3 followed by ReLU ac- \\ntivation functions. 3 × 3 max-pooling layers added just after every ﬁrst ﬁve convolu- \\ntional layers and average pooling at the last convolution layer. Every convolutional \\nlayer has two fully-connected layers and 200 hidden units. \\n\\nFig. 3.  Hybrid CNN-RNN Network Architecture. \\n\\n3.3. Combining with recurrent neural networks (RNNs)'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='3.3. Combining with recurrent neural networks (RNNs) \\n\\nIn the proposed model like the model which presented by [31] , \\nwe propose to combine the sequential information by using RNN \\nto spread information. The CNN model used for feature extraction \\nto ﬁx all of its parameters and to eliminate the regression layer. \\nFor the processing, when the image passed to the network, 200- \\ndimensional vectors will be extracted from the fully-connected lay- \\ners. For the assumed time t, we take P frames from the past (i.e. \\n[ t − P, t ]). Then passes every frame from time t − P to t to the CNN \\nand extract P vectors fully for each image. Each and every vector \\ngoes through a node of the RNN model. Then every node of the \\nRNN returns some results of valence label. The overall proposed \\nmethod illustrated in Fig. 3 . The mean squared error has been used \\nfor the cost function while optimizing. \\n\\n4. Experiment and evaluation'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='4. Experiment and evaluation \\n\\nFor the data preprocessing, we initially identify the face in ev- \\nery outline utilizing face and point of interest ﬁnder. Then map the \\ndistinguished landmark points to characterized pixel areas in a re- \\nquest to guarantee correspondence concerning outlines. After the \\nnormalization the nose, mouth and nose organizes, while process- \\ning each face image through the CCN mean subtraction and con- \\ntrast normalization applied. We tested the proposed models on a \\nnormal PC with Intel(R) Core(TM) i7-8700 K and 24 GB of RAM. \\n\\n4.1. Compare the CNN with hybrid CNN-RNN \\n\\nFig. 4 shows the loss and the prediction accuracy of the Hybrid \\nCNN-RNN model for training vas validation for one set of the Im- \\nages. These charts clearly illustrate the smooth performance of the \\nproposed model.'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='Table 1 presents the prediction accuracy of the proposed sin- \\ngle frame regression CNN and Hybrid CNN-RNN technique imple- \\nmented for predicting valence scores of subjects to developing a \\nset  of  the  dataset.  Finally,  when  combining  the  information  and \\nusing the Hybrid CNN-RNN model with the ReLU, a signiﬁcant per- \\nformance could be achived. \\n\\nFig. 4.  Loss and the Prediction Accuracy for Hybrid CNN-RNN model. \\n\\nTable 1 \\nOverall accuracy and mean accuracy for the different models. \\n\\nMethod \\n\\nOverall accuracy  Mean class accuracy \\n\\nCNN \\nCNN - RNN \\nCNN - RNN + ReLU \\n\\n76.51% \\n91.20% \\n94.46% \\n\\n74.33% \\n89.13% \\n93.67% \\n\\nFig. 5.  Roc and the Precision-Recall Curve. \\n\\nTable 2 \\nResult of altering the number of hidden units. \\n\\nMethod \\n\\nPrediction accuracy \\n\\nLoss \\n\\nHybrid CNN-RNN, hidden units = 50 \\nHybrid CNN-RNN, hidden units = 100 \\nHybrid CNN-RNN, hidden units = 150 \\nHybrid CNN-RNN, hidden units = 200 \\n\\n92.32% \\n93.57% \\n94.21% \\n92.53% \\n\\n4.73% \\n4.72% \\n4.43% \\n4.68%'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='92.32% \\n93.57% \\n94.21% \\n92.53% \\n\\n4.73% \\n4.72% \\n4.43% \\n4.68% \\n\\nFig. 5 displays the Roc curve and the Precision-Recall curve of \\nthe proposed hybrid model. As it is visible the proposed model has \\nthe ability to with the  least number of  errors  used for the face \\nemotion detection. \\n\\nWe evaluated the special effects of two hyperparameters in the \\nresults of Hybrid proposed model, namely the number of hidden \\nunits  and  the  number  of  hidden  layers.  Table  2  concluded  that, \\nthe best result can achieve with 150 hidden units and in the other \\ncases rather than improvement in the performance resulted in de- \\ncreases. Table 3 “shows that increasing the number of hidden lay- \\ners resulted to improve the overall performance of the proposed \\n\\n\\x0cN. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106 \\n\\n105 \\n\\nTable 3 \\nResult of altering the number of hidden layers. \\n\\nMethod \\n\\nPrediction accuracy \\n\\nLoss'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='Table 3 \\nResult of altering the number of hidden layers. \\n\\nMethod \\n\\nPrediction accuracy \\n\\nLoss \\n\\nHybridCNN-RNN, hidden layers = 4 \\nHybridCNN-RNN, hidden layers = 5 \\nHybrid CNN-RNN, hidden layers = 6 \\n\\n94.57% \\n94.73% \\n94.91% \\n\\n4.28% \\n4.22% \\n3.98% \\n\\nFig. 6.  Confusion matrices on JAFFE Datasets. \\n\\nTable 4 \\nProposed model versus other models performance comparison on \\nJAFFE and MMI dataset. \\n\\nMethod \\n\\nAccuracy of JAFFE \\n\\nAccuracy of MMI \\n\\nZhang et al. [30] \\nKhorrami et al. [31] \\nChernykh et al. [32] \\nFan et al. [33] \\nProposed model \\n\\n94.89% \\n82.43% \\n73% \\n79.16% \\n94.91% \\n\\n91.83% \\n81.48% \\n70.12% \\n77.83% \\n92.07% \\n\\nmodel. Hence, based on the experiments, the best results obtained \\nby the 6 hidden layers”.'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='model. Hence, based on the experiments, the best results obtained \\nby the 6 hidden layers”. \\n\\nThe confusion matrices of CNN and Hybrid CNN-RNN models \\non the testing sets are presented in Fig. 6 . Hybrid CNN-RNN model \\ncould achieve an accuracy of 94.72%, while a single CNN can reach \\nonly to 71.42%. The combined model not only increases the overall \\naccuracy of the proposed CNN model but also it reduces the false \\ndetection of the model. As it is clearly visible in the Fig. 6 the best \\ndetection are for the Ang, Neu, and Sur emotions. \\n\\nTable 4 indications the performance of proposed Hybrid CNN- \\nRNN model in comparison with other approaches evaluated on the \\nJAFFE and MMI datasets. The proposed CNN-RNN model achieved \\nequal or greater performance as compared to the four other state- \\nof-the-art methods [30–33] . \\n\\n4.2. Comparison of the proposed model with other approaches'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='4.2. Comparison of the proposed model with other approaches \\n\\nOur  model  has  slightly  better  performance  than  the  model \\nwhich proposed by Zhang et al. [30] , While, the other models have \\nthe lower performance in comparison with the proposed model. \\n\\n5. Conclusion \\n\\nIn  this  paper,  a  model  has  been  proposed  for  face  emotion \\nrecognition. We proposed a hybrid deep CNN and RNN model. In \\naddition,  the  proposed  model  evaluated  under  different  circum- \\nstances  and  hyper  parameters  to  properly  tuning  the  proposed \\nmodel. Particularly, it has been found that the combination of the \\ntwo types of neural networks (CNN-RNN) cloud signiﬁcantly im- \\nprove the overall result of detection, which veriﬁed the eﬃciency \\nof the proposed model. \\n\\nReferences \\n\\n[1] P. Ekman , W.V. Friesen , Constants across cultures in the face and emotion, J. \\n\\nPers. Soc. Psychol. 17 (2) (1971) 124 .'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='Pers. Soc. Psychol. 17 (2) (1971) 124 . \\n\\n[2] S.E. Kahou , P. Froumenty , C. Pal , Facial expression analysis based on high di- \\nmensional binary features, ECCV Workshop on Computer Vision with Local Bi- \\nnary Patterns Variants, 2014 . \\n\\n[3] C. Shan , S. Gong , P.W. McOwan , Facial expression recognition based on local \\nbinary patterns: a comprehensive study, Image Vis. Comput. 27 (6) (May 2009) \\n803–816 . \\n\\n[4] N. Kalchbrenner, E. Grefenstette, and P. Blunsom. A convolutional neural net- \\n\\nwork for modelling sentences. arXiv: 1404.2188 , 2014. \\n\\n[5] A. Krizhevsky , I. Sutskever , G.E. Hinton , Imagenet classiﬁcation with deep con- \\nvolutional neural networks, Adv. Neural Inf. Process. Syst. (2012) 1097–1105 . \\n[6] S.E. Kahou , C. Pal , X. Bouthillier , P. Froumenty , C. Gulcehre , et al. , Combining \\nmodality speciﬁc deep neural networks for emotion recognition in video, In- \\nternational Conference on Multimodal Interaction, ICMI ’13, 2013 .'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='[7] M. Liu , R. Wang , S. Li , S. Shan , Z. Huang , X. Chen , Combining multiple kernel \\nmethods on riemannian manifold for emotion recognition in the wild, in: In- \\nternational Conference on Multimodal Interaction, ICMI ’14, 2014, pp. 494–501 . \\n[8] S.E. Kahou , X. Bouthillier , P. Lamblin , C. Gulcehre , V. Michalski , et al. , Emon- \\nets: multimodal deep learning approaches for emotion recognition in video, J. \\nMultimodal User Interfaces (2015) 1–13 . \\n\\n[9] E. Sariyanidi , H. Gunes , A. Cavallaro , Automatic analysis of facial affect: a sur- \\nvey of registration, representation, and recognition, IEEE Trans. Pattern Anal. \\nMach. Intell. 37 (6) (2015) 1113–1133 . \\n\\n[10] C. Shan , S. Gong , P.W. McOwan , Facial expression recognition based on lo- \\ncal binary patterns: a comprehensive study, Image Vis. Comput. 27 (6) (2009) \\n803–816 .'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='[11] N. Dalal , B. Triggs , Histograms of oriented gradients for human detection, in: \\nComputer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer \\nSociety Conference on, 1, IEEE, 2005, pp. 886–893 . \\n\\n[12] T.F. Cootes , G.J. Edwards , C.J. Taylor , et al. , Active appearance models, IEEE \\n\\nTrans. Pattern Anal. Mach. Intell. 23 (6) (2001) 6 81–6 85 . \\n\\n[13] M. Yeasin , B. Bullot , R. Sharma , Recognition of facial expressions and measure- \\nment of levels of interest from video, Multimedia, IEEE Trans. 8 (3) (2006) \\n500–508 . \\n\\n[14] Y. Zhu , L.C. De Silva , C.C. Ko , Using moment invariants and hmm in facial ex- \\n\\npression recognition, Pattern Recognit. Lett. 23 (1) (2002) 83–91 . \\n\\n[15] Y. Sun , X. Chen , M. Rosato , L. Yin , Tracking vertex ﬂow and model adaptation \\nfor three-dimensional spatiotemporal face analysis, Syst. Man Cybern. Part A \\n40 (3) (2010) 461–474 . \\n\\n[16] N. Sebe , M.S. Lew , Y. Sun , I. Cohen , T. Gevers , T.S. Huang , Authentic facial ex-'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='[16] N. Sebe , M.S. Lew , Y. Sun , I. Cohen , T. Gevers , T.S. Huang , Authentic facial ex- \\n\\npression analysis, Image Vis. Comput. 25 (12) (2007) 1856–1863 . \\n\\n[17] B. Hasani , M.M. Arzani , M. Fathy , K. Raahemifar , Facial expression recognition \\nwith discriminatory graphical models, in: 2016 2nd International Conference \\nof Signal Processing and Intelligent Systems (ICSPIS), Dec 2016, pp. 1–7 . \\n[18] B. Hasani and M.H. Mahoor. Spatio-temporal facial expression recognition us- \\ning convolutional neural networks and conditional random ﬁelds. arXiv: 1703. \\n06995 , 2017. \\n\\n[19] A. Krizhevsky , I. Sutskever , G.E. Hinton , Imagenet classiﬁcation with deep con- \\n\\nvolutional neural networks, Adv. Neural Inf. Process. Syst. (2012) 1097–1105 .'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='volutional neural networks, Adv. Neural Inf. Process. Syst. (2012) 1097–1105 . \\n\\n[20] C. Szegedy , W. Liu , Y. Jia , P. Sermanet , S. Reed , D. Anguelov , D. Erhan , V. Van- \\nhoucke , A. Rabinovich , Going deeper with convolutions, in: Proceedings of the \\nIEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1–9 . \\n\\n[21] A. Mollahosseini , B. Hasani , M.J. Salvador , H. Abdollahi , D. Chan , M.H. Mahoor , \\nFacial expression recognition from world wild web, In The IEEE Conference on \\nComputer Vision and Pattern Recognition (CVPR) Workshops, June 2016 . \\n[22] S. Ioffe and C. Szegedy. Batch normalization: accelerating deep network train- \\n\\ning by reducing internal covariate shift. arXiv: 1502.03167 , 2015. \\n\\n[23] A . Graves , A . r. Mohamed , G. Hinton , Speech recognition with deep recurrent \\nneural networks, in: Proc. IEEE International Conference on Acoustics, Speech \\nand Signal Processing, 2013, pp. 6645–6649 . \\n\\n\\x0c106'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='106 \\n\\nN. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106 \\n\\n[24] A. Graves , N. Jaitly , Towards end-to-end speech recognition with recurrent \\nneural networks, in: Proc. International Conference on Machine Learning, 2014, \\npp. 1764–1772 . \\n\\n[25] T.  Mikolov ,  M.  Karaﬁ ´at ,  L.  Burget ,  J.  Cernock  ‘y ,  S.  Khudanpur , Recurrent \\nneural  network  based  language  model,  in:  Proc.  INTERSPEECH,  2,  2010, \\npp. 1045–1048 . \\n\\n[26] A. Sanin , C. Sanderson , M.T. Harandi , B.C. Lovell , Spatiotemporal covariance de- \\nscriptors for action and gesture recognition, IEEE Workshop on Applications of \\nComputer Vision, 2013 . \\n\\n[27] S. Jain , C. Hu , J.K. Aggarwal , Facial expression recognition with temporal mod- \\neling of shapes, in: Proc. IEEE International Conference on Computer Vision \\nWorkshops, 2011, pp. 1642–1649 .'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='[28] F. Visin, K. Kastner, K. Cho, M. Matteucci, et al., Renet: A recurrent neural net- \\nwork based alternative to convolutional networks. arXiv: 1505.00393 , 2015 \\n[29] F. Visin, K. Kastner, A. Courville, Y. Bengio, et al., ReSeg: a recurrent neural \\n\\nnetwork for object segmentation. arXiv: 1511.07053 , 2015 \\n\\n[30] T. Zhang, W. Zheng, Z. Cui, Y. Zong, Y. Li, Spatial-temporal recurrent neu- \\nral  network  for  emotion  recognition,  IEEE  Trans.  Cybern.  (99)  (2018)  1–9 \\narXiv: 1705.04515 . \\n\\n[31] P. Khorrami , T.L. Paine , K. Brady , C. Dagli , T.S. Huang , How Deep Neural Net- \\nworks can Improve Emotion Recognition on Video Data, IEEE Conf. Image Pro- \\ncess (ICIP) (2016) . \\n\\n[32] V. Chernykh, G. Sterling, P. Prihodko, Emotion recognition from speech with \\n\\nrecurrent neural networks, arXiv: 1701.08071v1 [cs.CL], 2017'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='recurrent neural networks, arXiv: 1701.08071v1 [cs.CL], 2017 \\n\\n[33] Y. Fan , X. Lu , D. Li , Y. Liu , Video-based emotion recognition using CNN-RNN \\nand C3D hybrid networks, in: ACM International Conference on Multimodal \\nInteraction (ICMI 2016), 2016, pp. 445–450 . \\n\\n[34] S. Hochreiter , J. Schmidhuber , Long short-term memory, Neural Comput. 9 (8) \\n\\n(1997) 1735–1780 . \\n\\n[35] J.  Donahue,  L.A.  Hendricks,  S.  Guadarrama,  M.  Rohrbach,  S.  Venugopalan, \\nK. Saenko, T. Darrell, Long-Term Recurrent Convolutional Networks for Visual \\nRecognition and Description, IEEE Trans. Pattern Anal. Mach. Intell. 39 (4) \\n(2017) 677–691, doi: 10.1109/TPAMI.2016.2599174 . \\n\\n[36] Q.V. Le, N. Jaitly, and G.E. Hinton. A simple way to initialize recurrent networks \\n\\nof rectiﬁed linear units. arXiv: 1504.00941 , 2015. \\n\\n[37] J. Susskind, A. Anderson, and G. Hinton. The toronto face database. Technical \\n\\nreport, UTML TR 2010-001, University of Toronto, 2010.'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='report, UTML TR 2010-001, University of Toronto, 2010. \\n\\n[38] M.J.  Lyons,  J.  Budynek,  S.  Akamatsu,  Automatic  classiﬁcation  of  single  fa- \\ncial images, IEEE Trans. Pattern Anal. Mach. Intell. 21 (12) (1999) 1357–1362, \\ndoi: 10.1109/34.817413 . \\n\\n[39] A. Dhall , R. Goecke , J. Joshi , K. Sikka , T. Gedeon , Emotion recognition in the \\nwild challenge 2014: baseline, data and protocol, in: International Conference \\non Multimodal Interaction, ICMI ’14, 2014, pp. 461–466 . \\n\\n[40] Y. Sun , X. Wang , X. Tang , Deep convolutional network cascade for facial point \\ndetection, in: IEEE Conference on Computer Vision and Pattern Recognition, \\nCVPR ’13, 2013, pp. 3476–3483 .'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='[41] I. Sutskever , J. Martens , G. Dahl , G. Hinton , On the importance of initialization \\nand momentum in deep learning, in: Proceedings of the 30th International \\nConference on Machine Learning, 2013, pp. 1139–1147. D. P. Kingma and J. Ba. \\nAdam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014 . \\n[42] Xie S. , Hu H. , Facial expression recognition with FRR – CNN, Electron. Lett. 53 \\n\\n(4) (2017) 235–237 . \\n\\n[43] S. Ouellet, Real-time emotion recognition for gaming using deep convolutional \\n\\nnetwork features, CoRR, vol. abs/1408.3750, 2014. \\n\\n[44] G. Levi , T. Hassner , Emotion recognition in the wild via convolutional neural \\nnetworks and mapped binary patterns, in: Proc. ACM International Conference \\non Multimodal Interaction (ICMI), November, 2015 . \\n\\n[45] D.K. Jain , R. Kumar , N. Jain , Decision-based spectral embedding approach for \\nidentifying facial behaviour on RGB-D images, Int. Conf. Commun. Netw. 508 \\n(2017) 677–687 .'), Document(metadata={'Path': 'home/path1/paper1/book1', 'fileName': 'page1Researchpaper'}, page_content='[46] D.K. Jain , Z. Zhang , K. Huang , Hybrid patch based diagonal pattern geometric \\nappearance model for facial expression recognition, in: Conference on Intelli- \\ngent Visual Surveillance, 2016, pp. 107–113 . \\n\\n[47] D.K. Jain, Z. Zhang, K. Huang, Multi angle optimal pattern-based deep learn- \\ning for automatic facial expression recognition, Pattern Recognit. Lett. (2017), \\ndoi: 10.1016/j.patrec.2017.06.025 . \\n\\n[48] D.K. Jain, Z. Zhang, K. Huang, Random walk-based feature learning for micro- \\nexpression recognition, 2018. https://doi.org/10.1016/j.patrec.2018.02.004 .')]\n"
     ]
    }
   ],
   "source": [
    "print(documenData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "d:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First embedding vector: [-0.1435166448354721, -0.06352461874485016, 0.12745055556297302, 0.017167899757623672, 0.05016566440463066, 0.0720740407705307, -0.07063586264848709, -0.013578503392636776, -0.038224563002586365, -0.10427562147378922, -0.06044911965727806, -0.1026877835392952, -0.024967044591903687, 0.031248584389686584, -0.03757117688655853, -0.01171806175261736, -0.06882323324680328, 0.0002360045036766678, -0.03111574612557888, 0.0033957427367568016, 0.004007552284747362, 0.02496926113963127, -0.03862369805574417, -0.04590871185064316, 0.0030945073813199997, 0.07486848533153534, 0.008063464425504208, -0.007321974262595177, 0.001311782980337739, 0.049016281962394714, 0.0879964530467987, 0.03928418457508087, 0.04286695644259453, 0.07339965552091599, -0.01804019883275032, 0.03649494796991348, -0.10038410127162933, -0.026657309383153915, -0.011059544049203396, -0.017373405396938324, -0.03375544026494026, -0.037457846105098724, 0.02595609612762928, -0.02595820650458336, 0.13558952510356903, 0.0971478745341301, 0.009161806665360928, -0.032340675592422485, 0.018458357080817223, -0.05649114027619362, -0.01261270884424448, -0.017096247524023056, -0.021600935608148575, 0.03481604531407356, -0.0049425349570810795, -0.04497114196419716, 0.012459165416657925, -0.04200219362974167, -0.008723022416234016, 0.010158347897231579, 0.061786849051713943, -0.008072473108768463, -0.017578179016709328, 0.006536440458148718, -0.06461481750011444, 0.04844358190894127, -0.025817127898335457, 0.006502595264464617, 0.07283887267112732, -0.04076451063156128, 0.007936924695968628, 0.10486886650323868, 0.026531165465712547, -0.031294144690036774, -0.03371460735797882, 0.03833648934960365, 0.03340150788426399, -0.025301510468125343, 0.0854412242770195, -0.01387922465801239, 0.04899537190794945, -0.009229746647179127, 0.07206648588180542, -0.05166570097208023, 0.011162487789988518, -0.08979786187410355, -0.10132329165935516, -0.01662132330238819, -0.07635550945997238, -0.0012222034856677055, -0.005485008005052805, -0.03984835743904114, 0.035528216511011124, -0.08956380188465118, -0.03827575594186783, -0.027483386918902397, -0.06476712971925735, -0.0009952832479029894, -0.08158621937036514, 0.01651807874441147, -0.05335400998592377, -0.04219868406653404, -0.05054439604282379, 0.07695623487234116, -0.015346036292612553, -0.05870066210627556, -0.018611401319503784, -0.04266636446118355, 0.10318964719772339, -0.08101911842823029, -0.10880713164806366, 0.01452622003853321, -0.0423181876540184, -0.05567466467618942, 0.019915807992219925, -0.06941907107830048, 0.00037772691575810313, -0.05457688868045807, 0.07372632622718811, 0.0634908378124237, -0.03509613871574402, -0.07425354421138763, -0.0032948015723377466, -0.00024148606462404132, 0.07551438361406326, 0.01211448572576046, -0.08041555434465408, 8.558642859906577e-33, 0.04090581461787224, 0.022132864221930504, 0.011478674598038197, -0.015752844512462616, 0.010658521205186844, -0.015164840966463089, -0.05163617059588432, -0.03950755298137665, -0.04932546615600586, 0.006610001903027296, -0.13785536587238312, 0.08786794543266296, -0.07706982642412186, 0.1409808248281479, -0.023640962317585945, -0.025594865903258324, 0.038737952709198, -0.08209685981273651, 0.008871992118656635, -0.11624164879322052, -0.08731551468372345, 0.0016062995418906212, 0.04190969839692116, 0.06352856010198593, -0.04927981644868851, 0.026757456362247467, 0.08503827452659607, -0.055659666657447815, 0.009560932405292988, -0.019465522840619087, 0.006447911728173494, -0.028613941743969917, 0.02353494241833687, -0.01731755957007408, 0.08689865469932556, -0.022334491834044456, -0.03961620107293129, -0.003607657738029957, 0.05993307754397392, -0.0078422287479043, -0.00906634982675314, 0.036811552941799164, -0.024652114138007164, -0.0067346710711717606, 0.029115701094269753, 0.04852841794490814, 0.04180004820227623, 0.04019124060869217, 0.05767107382416725, 0.021796315908432007, 0.018574418500065804, 0.023588070645928383, -0.07767322659492493, -0.04287775605916977, 0.029211463406682014, 0.005556376650929451, 0.05167004093527794, 0.06229477748274803, -0.032012734562158585, -0.04050289839506149, 0.015831440687179565, -0.04705518111586571, 0.024023061618208885, 0.009846088476479053, 0.026274293661117554, 0.006923450622707605, -0.0666017085313797, 0.02538852021098137, -0.024819297716021538, 0.0427788607776165, 0.08955782651901245, 0.0810348242521286, 0.02678714506328106, -0.06329528987407684, 0.036692481487989426, 0.035018157213926315, 0.06189607456326485, 0.012595242820680141, -0.06908504664897919, 0.04461492598056793, -0.018892835825681686, 0.08238671720027924, -0.023151656612753868, -0.04554787278175354, -0.05917033553123474, 0.015664685517549515, 0.08587133884429932, -0.07412134110927582, -0.02311851643025875, 0.029954958707094193, 0.03274598345160484, -0.036827486008405685, 0.07827141135931015, 0.002082462888211012, -0.06777440011501312, -9.756231361736539e-33, 0.03196744620800018, 0.024487553164362907, -0.0765359103679657, -0.046052198857069016, 0.008739610202610493, 0.00304793962277472, -0.013075356371700764, 0.011909599415957928, 0.016719257459044456, 0.05457238852977753, 0.08733267337083817, -0.03331177309155464, 0.08889352530241013, -0.03173904865980148, 0.009913659654557705, -0.039307039231061935, -0.012030267156660557, 0.022295909002423286, 0.009711459279060364, 0.03895611688494682, -0.01490065362304449, 0.013940931297838688, -0.08501178026199341, -0.0008680114988237619, -0.08456023037433624, -0.017264414578676224, 0.039190977811813354, 0.05726458877325058, -0.014626845717430115, -0.01168542355298996, -0.10342904925346375, 0.030571332201361656, -0.09026267379522324, 0.08275425434112549, -0.019335657358169556, 0.06696178764104843, -0.028328189626336098, -0.06524089723825455, -0.12315452098846436, 0.016482148319482803, 0.08575557917356491, 0.020656587556004524, -0.06822887808084488, 0.011328588239848614, -0.018339619040489197, -0.06982202082872391, -0.07001243531703949, 0.02186388522386551, 0.015916990116238594, 0.012153634801506996, 0.050080668181180954, -0.003916944842785597, -0.07641038298606873, 0.061172228306531906, 0.011715173721313477, 0.024799128994345665, -0.01171368919312954, 0.02291271463036537, -0.014336250722408295, 0.02136710286140442, -0.0028251244220882654, -0.034793831408023834, -0.05522136017680168, -0.0030406005680561066, -0.03672437742352486, 0.04812248423695564, -0.027612825855612755, 0.028341826051473618, 0.02433343045413494, 0.0015999095048755407, 0.059890225529670715, 0.03334064409136772, 0.02416246198117733, 0.026961009949445724, -0.0774332731962204, -0.08787629008293152, -0.07151775807142258, -0.020154055207967758, -0.053417790681123734, -0.0010780930751934648, -0.036436665803194046, -0.07801184058189392, -0.005452351178973913, 0.09271673858165741, 0.08178473263978958, 0.047317422926425934, -0.017239119857549667, 0.029444420710206032, 0.08264654129743576, 0.019040806218981743, -0.01019811350852251, 0.05515201389789581, 0.00751759298145771, 0.038821030408144, 0.006587561685591936, -5.0207525248424645e-08, -0.029989734292030334, -0.03441474959254265, 0.013839411549270153, -0.05227358639240265, 0.0043936483561992645, -0.07910545915365219, 0.05665680393576622, -0.04284433275461197, -0.05163557082414627, -0.011765247210860252, 0.10550718009471893, 0.04113328084349632, -0.015157395042479038, -0.05107717961072922, 0.0018515135161578655, 0.07543295621871948, 0.03832745924592018, -0.016373837366700172, 0.031559016555547714, 0.0037919010501354933, 0.03452693298459053, 0.01867990754544735, -0.017093997448682785, -0.0042940713465213776, 0.02701869234442711, 0.002568814903497696, -0.07339698821306229, 0.08481002599000931, -0.007859929464757442, -0.03266741707921028, 0.01429496705532074, 0.057701218873262405, 0.05711976811289787, -0.13610650599002838, 0.08519087731838226, 0.08043118566274643, 0.030970532447099686, -0.003597891889512539, 0.002927974332123995, 0.03037225268781185, -0.03007325530052185, 0.026729963719844818, -0.05361137539148331, 0.042335812002420425, -0.044254008680582047, 0.03934876248240471, 0.05689472705125809, -0.07714088261127472, 0.08439340442419052, 0.03031953237950802, 0.08147194981575012, -0.018133798614144325, -0.007614064961671829, 0.12362854182720184, -0.012059141881763935, -0.017277495935559273, 0.0025479274336248636, -0.030358577147126198, 0.059647656977176666, 0.0792304277420044, 0.011128649115562439, -0.03049405850470066, -0.0829242467880249, 0.01525953784584999]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.utils import embedding_functions\n",
    "from typing import List\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "model1 = SentenceTransformer('all-MiniLM-L6-v2')  # A good lightweight model\n",
    "\n",
    "\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model1.encode(chunks).tolist()\n",
    "\n",
    "\n",
    "\n",
    "print(\"First embedding vector:\", embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in the collection:  47\n"
     ]
    }
   ],
   "source": [
    "all_documents = vectorstore.get()['documents']\n",
    "total_records = len(all_documents)\n",
    "print(\"Total records in the collection: \", total_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = vectorstore.()\n",
    "for doc in all_docs:\n",
    "    print(\"Text:\", doc[\"text\"])\n",
    "    print(\"Metadata:\", doc[\"metadata\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The activation functions used in this regression CNN are ReLU (Rectified Linear Unit) for both convolutional and fully-connected layers, as well as dropout for training.\n",
      "Answer: They had 3 trials.\n",
      "Answer: I don't know what the question is based on the provided context, and I'm not aware of any specific paper or study that would be referenced in the DOI and authors. Can you please provide more information or clarify the context?\n",
      "Answer: The question refers to a trial, but the specific context provided is about neural networks and their application in machine learning and artificial intelligence. Since I don't have information on this particular topic or a detailed explanation of what's being asked, I'll say that I don't know without providing an incorrect answer.\n",
      "Answer: The trials mentioned in the passage are likely experiments or studies conducted to evaluate different approaches, algorithms, or techniques used in Facial Expression Recognition (FER). These trials may involve testing various input modalities, such as video footage, audio records, or speech recognition with video features. The goal is to identify which methods perform best and understand why some have limited success in accurately identifying human emotions.\n",
      "Exiting program.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\nQuery: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"Exiting program.\")\n",
    "        break\n",
    "    if query.strip() == \"\":\n",
    "        print(\"Please enter a valid query.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Prompt\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use three sentences maximum and keep the answer as concise as possible.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "   \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"query\": query})\n",
    "\n",
    "    print(\"Answer:\", result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "# Prompt template for the QA chain\n",
    "template = \"\"\"Use the following context to answer the question.\n",
    "If you don't know the answer, say you don't know. Be concise.\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),  # Using vectorstore as the retriever\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The document appears to be a research paper, but I cannot verify the accuracy of its claims or conclusions without further context or information.\n"
     ]
    }
   ],
   "source": [
    "# Run a query through the QA chain\n",
    "query = \"What are the accuracies found in this document?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result[\"result\"])  # Output the answer generated by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amedo\\AppData\\Local\\Temp\\ipykernel_19404\\947014547.py:2: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={}, page_content='∗ Corresponding author. \\n\\nE-mail address: neha.juet@gmail.com (N. Jain). \\n\\nhttps://doi.org/10.1016/j.patrec.2018.04.010 \\n0167-8655/© 2018 Elsevier B.V. All rights reserved.'), -0.09527571563660664), (Document(metadata={}, page_content='∗ Corresponding author. \\n\\nE-mail address: neha.juet@gmail.com (N. Jain). \\n\\nhttps://doi.org/10.1016/j.patrec.2018.04.010 \\n0167-8655/© 2018 Elsevier B.V. All rights reserved.'), -0.09527571563660664), (Document(metadata={}, page_content='© 2018 Elsevier B.V. All rights reserved. \\n\\n1. Introduction'), -0.16326802317874511), (Document(metadata={}, page_content='© 2018 Elsevier B.V. All rights reserved. \\n\\n1. Introduction'), -0.16326802317874511), (Document(metadata={}, page_content='Table 3 \\nResult of altering the number of hidden layers. \\n\\nMethod \\n\\nPrediction accuracy \\n\\nLoss \\n\\nHybridCNN-RNN, hidden layers = 4 \\nHybridCNN-RNN, hidden layers = 5 \\nHybrid CNN-RNN, hidden layers = 6 \\n\\n94.57% \\n94.73% \\n94.91% \\n\\n4.28% \\n4.22% \\n3.98% \\n\\nFig. 6.  Confusion matrices on JAFFE Datasets. \\n\\nTable 4 \\nProposed model versus other models performance comparison on \\nJAFFE and MMI dataset. \\n\\nMethod \\n\\nAccuracy of JAFFE \\n\\nAccuracy of MMI \\n\\nZhang et al. [30] \\nKhorrami et al. [31] \\nChernykh et al. [32] \\nFan et al. [33] \\nProposed model \\n\\n94.89% \\n82.43% \\n73% \\n79.16% \\n94.91% \\n\\n91.83% \\n81.48% \\n70.12% \\n77.83% \\n92.07% \\n\\nmodel. Hence, based on the experiments, the best results obtained \\nby the 6 hidden layers”.'), -0.17263339009265377), (Document(metadata={}, page_content='Table 3 \\nResult of altering the number of hidden layers. \\n\\nMethod \\n\\nPrediction accuracy \\n\\nLoss \\n\\nHybridCNN-RNN, hidden layers = 4 \\nHybridCNN-RNN, hidden layers = 5 \\nHybrid CNN-RNN, hidden layers = 6 \\n\\n94.57% \\n94.73% \\n94.91% \\n\\n4.28% \\n4.22% \\n3.98% \\n\\nFig. 6.  Confusion matrices on JAFFE Datasets. \\n\\nTable 4 \\nProposed model versus other models performance comparison on \\nJAFFE and MMI dataset. \\n\\nMethod \\n\\nAccuracy of JAFFE \\n\\nAccuracy of MMI \\n\\nZhang et al. [30] \\nKhorrami et al. [31] \\nChernykh et al. [32] \\nFan et al. [33] \\nProposed model \\n\\n94.89% \\n82.43% \\n73% \\n79.16% \\n94.91% \\n\\n91.83% \\n81.48% \\n70.12% \\n77.83% \\n92.07% \\n\\nmodel. Hence, based on the experiments, the best results obtained \\nby the 6 hidden layers”.'), -0.17263339009265377), (Document(metadata={}, page_content='(2) \\n\\nFirstly  we  used  a  single  CNN  model  to  train  the  datasets.  At \\neach time trained a single image, the corresponding image passed \\nthrough the CNN model, the details of the model shown in Fig. 2 . \\nTwo fully-connected layers with 200 hidden units for the ap- \\nproximation of the valence label have been used. For the cost func- \\ntion the mean squared error has been used. For the network train- \\ning stochastic gradient descent while the batch size sets to 32 and \\nthe weight decay sets to 1E-4. Moreover, the learning rate at the \\nbeginning sets to 5e-3 which decrees by 0.01 every 20 epochs. \\n\\n\\x0c104 \\n\\nN. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106'), -0.20933570871291063), (Document(metadata={}, page_content='(2) \\n\\nFirstly  we  used  a  single  CNN  model  to  train  the  datasets.  At \\neach time trained a single image, the corresponding image passed \\nthrough the CNN model, the details of the model shown in Fig. 2 . \\nTwo fully-connected layers with 200 hidden units for the ap- \\nproximation of the valence label have been used. For the cost func- \\ntion the mean squared error has been used. For the network train- \\ning stochastic gradient descent while the batch size sets to 32 and \\nthe weight decay sets to 1E-4. Moreover, the learning rate at the \\nbeginning sets to 5e-3 which decrees by 0.01 every 20 epochs. \\n\\n\\x0c104 \\n\\nN. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106'), -0.20933570871291063), (Document(metadata={}, page_content='Table 1 presents the prediction accuracy of the proposed sin- \\ngle frame regression CNN and Hybrid CNN-RNN technique imple- \\nmented for predicting valence scores of subjects to developing a \\nset  of  the  dataset.  Finally,  when  combining  the  information  and \\nusing the Hybrid CNN-RNN model with the ReLU, a signiﬁcant per- \\nformance could be achived. \\n\\nFig. 4.  Loss and the Prediction Accuracy for Hybrid CNN-RNN model. \\n\\nTable 1 \\nOverall accuracy and mean accuracy for the different models. \\n\\nMethod \\n\\nOverall accuracy  Mean class accuracy \\n\\nCNN \\nCNN - RNN \\nCNN - RNN + ReLU \\n\\n76.51% \\n91.20% \\n94.46% \\n\\n74.33% \\n89.13% \\n93.67% \\n\\nFig. 5.  Roc and the Precision-Recall Curve. \\n\\nTable 2 \\nResult of altering the number of hidden units. \\n\\nMethod \\n\\nPrediction accuracy \\n\\nLoss \\n\\nHybrid CNN-RNN, hidden units = 50 \\nHybrid CNN-RNN, hidden units = 100 \\nHybrid CNN-RNN, hidden units = 150 \\nHybrid CNN-RNN, hidden units = 200 \\n\\n92.32% \\n93.57% \\n94.21% \\n92.53% \\n\\n4.73% \\n4.72% \\n4.43% \\n4.68%'), -0.21027667925276483), (Document(metadata={}, page_content='Table 1 presents the prediction accuracy of the proposed sin- \\ngle frame regression CNN and Hybrid CNN-RNN technique imple- \\nmented for predicting valence scores of subjects to developing a \\nset  of  the  dataset.  Finally,  when  combining  the  information  and \\nusing the Hybrid CNN-RNN model with the ReLU, a signiﬁcant per- \\nformance could be achived. \\n\\nFig. 4.  Loss and the Prediction Accuracy for Hybrid CNN-RNN model. \\n\\nTable 1 \\nOverall accuracy and mean accuracy for the different models. \\n\\nMethod \\n\\nOverall accuracy  Mean class accuracy \\n\\nCNN \\nCNN - RNN \\nCNN - RNN + ReLU \\n\\n76.51% \\n91.20% \\n94.46% \\n\\n74.33% \\n89.13% \\n93.67% \\n\\nFig. 5.  Roc and the Precision-Recall Curve. \\n\\nTable 2 \\nResult of altering the number of hidden units. \\n\\nMethod \\n\\nPrediction accuracy \\n\\nLoss \\n\\nHybrid CNN-RNN, hidden units = 50 \\nHybrid CNN-RNN, hidden units = 100 \\nHybrid CNN-RNN, hidden units = 150 \\nHybrid CNN-RNN, hidden units = 200 \\n\\n92.32% \\n93.57% \\n94.21% \\n92.53% \\n\\n4.73% \\n4.72% \\n4.43% \\n4.68%'), -0.21027667925276483)]\n",
      "  results = vectorstore.similarity_search_with_relevance_scores(query, k=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 (Relevance Score: -0.09527571563660664):\n",
      "∗ Corresponding author. \n",
      "\n",
      "E-mail address: neha.juet@gmail.com (N. Jain). \n",
      "\n",
      "https://doi.org/10.1016/j.patrec.2018.04.010 \n",
      "0167-8655/© 2018 Elsevier B.V. All rights reserved.\n",
      "\n",
      "Document 2 (Relevance Score: -0.09527571563660664):\n",
      "∗ Corresponding author. \n",
      "\n",
      "E-mail address: neha.juet@gmail.com (N. Jain). \n",
      "\n",
      "https://doi.org/10.1016/j.patrec.2018.04.010 \n",
      "0167-8655/© 2018 Elsevier B.V. All rights reserved.\n",
      "\n",
      "Document 3 (Relevance Score: -0.16326802317874511):\n",
      "© 2018 Elsevier B.V. All rights reserved. \n",
      "\n",
      "1. Introduction\n",
      "\n",
      "Document 4 (Relevance Score: -0.16326802317874511):\n",
      "© 2018 Elsevier B.V. All rights reserved. \n",
      "\n",
      "1. Introduction\n",
      "\n",
      "Document 5 (Relevance Score: -0.17263339009265377):\n",
      "Table 3 \n",
      "Result of altering the number of hidden layers. \n",
      "\n",
      "Method \n",
      "\n",
      "Prediction accuracy \n",
      "\n",
      "Loss \n",
      "\n",
      "HybridCNN-RNN, hidden layers = 4 \n",
      "HybridCNN-RNN, hidden layers = 5 \n",
      "Hybrid CNN-RNN, hidden layers = 6 \n",
      "\n",
      "94.57% \n",
      "94.73% \n",
      "94.91% \n",
      "\n",
      "4.28% \n",
      "4.22% \n",
      "3.98% \n",
      "\n",
      "Fig. 6.  Confusion matrices on JAFFE Datasets. \n",
      "\n",
      "Table 4 \n",
      "Proposed model versus other models performance comparison on \n",
      "JAFFE and MMI dataset. \n",
      "\n",
      "Method \n",
      "\n",
      "Accuracy of JAFFE \n",
      "\n",
      "Accuracy of MMI \n",
      "\n",
      "Zhang et al. [30] \n",
      "Khorrami et al. [31] \n",
      "Chernykh et al. [32] \n",
      "Fan et al. [33] \n",
      "Proposed model \n",
      "\n",
      "94.89% \n",
      "82.43% \n",
      "73% \n",
      "79.16% \n",
      "94.91% \n",
      "\n",
      "91.83% \n",
      "81.48% \n",
      "70.12% \n",
      "77.83% \n",
      "92.07% \n",
      "\n",
      "model. Hence, based on the experiments, the best results obtained \n",
      "by the 6 hidden layers”.\n",
      "\n",
      "Document 6 (Relevance Score: -0.17263339009265377):\n",
      "Table 3 \n",
      "Result of altering the number of hidden layers. \n",
      "\n",
      "Method \n",
      "\n",
      "Prediction accuracy \n",
      "\n",
      "Loss \n",
      "\n",
      "HybridCNN-RNN, hidden layers = 4 \n",
      "HybridCNN-RNN, hidden layers = 5 \n",
      "Hybrid CNN-RNN, hidden layers = 6 \n",
      "\n",
      "94.57% \n",
      "94.73% \n",
      "94.91% \n",
      "\n",
      "4.28% \n",
      "4.22% \n",
      "3.98% \n",
      "\n",
      "Fig. 6.  Confusion matrices on JAFFE Datasets. \n",
      "\n",
      "Table 4 \n",
      "Proposed model versus other models performance comparison on \n",
      "JAFFE and MMI dataset. \n",
      "\n",
      "Method \n",
      "\n",
      "Accuracy of JAFFE \n",
      "\n",
      "Accuracy of MMI \n",
      "\n",
      "Zhang et al. [30] \n",
      "Khorrami et al. [31] \n",
      "Chernykh et al. [32] \n",
      "Fan et al. [33] \n",
      "Proposed model \n",
      "\n",
      "94.89% \n",
      "82.43% \n",
      "73% \n",
      "79.16% \n",
      "94.91% \n",
      "\n",
      "91.83% \n",
      "81.48% \n",
      "70.12% \n",
      "77.83% \n",
      "92.07% \n",
      "\n",
      "model. Hence, based on the experiments, the best results obtained \n",
      "by the 6 hidden layers”.\n",
      "\n",
      "Document 7 (Relevance Score: -0.20933570871291063):\n",
      "(2) \n",
      "\n",
      "Firstly  we  used  a  single  CNN  model  to  train  the  datasets.  At \n",
      "each time trained a single image, the corresponding image passed \n",
      "through the CNN model, the details of the model shown in Fig. 2 . \n",
      "Two fully-connected layers with 200 hidden units for the ap- \n",
      "proximation of the valence label have been used. For the cost func- \n",
      "tion the mean squared error has been used. For the network train- \n",
      "ing stochastic gradient descent while the batch size sets to 32 and \n",
      "the weight decay sets to 1E-4. Moreover, the learning rate at the \n",
      "beginning sets to 5e-3 which decrees by 0.01 every 20 epochs. \n",
      "\n",
      "\f104 \n",
      "\n",
      "N. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106\n",
      "\n",
      "Document 8 (Relevance Score: -0.20933570871291063):\n",
      "(2) \n",
      "\n",
      "Firstly  we  used  a  single  CNN  model  to  train  the  datasets.  At \n",
      "each time trained a single image, the corresponding image passed \n",
      "through the CNN model, the details of the model shown in Fig. 2 . \n",
      "Two fully-connected layers with 200 hidden units for the ap- \n",
      "proximation of the valence label have been used. For the cost func- \n",
      "tion the mean squared error has been used. For the network train- \n",
      "ing stochastic gradient descent while the batch size sets to 32 and \n",
      "the weight decay sets to 1E-4. Moreover, the learning rate at the \n",
      "beginning sets to 5e-3 which decrees by 0.01 every 20 epochs. \n",
      "\n",
      "\f104 \n",
      "\n",
      "N. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106\n",
      "\n",
      "Document 9 (Relevance Score: -0.21027667925276483):\n",
      "Table 1 presents the prediction accuracy of the proposed sin- \n",
      "gle frame regression CNN and Hybrid CNN-RNN technique imple- \n",
      "mented for predicting valence scores of subjects to developing a \n",
      "set  of  the  dataset.  Finally,  when  combining  the  information  and \n",
      "using the Hybrid CNN-RNN model with the ReLU, a signiﬁcant per- \n",
      "formance could be achived. \n",
      "\n",
      "Fig. 4.  Loss and the Prediction Accuracy for Hybrid CNN-RNN model. \n",
      "\n",
      "Table 1 \n",
      "Overall accuracy and mean accuracy for the different models. \n",
      "\n",
      "Method \n",
      "\n",
      "Overall accuracy  Mean class accuracy \n",
      "\n",
      "CNN \n",
      "CNN - RNN \n",
      "CNN - RNN + ReLU \n",
      "\n",
      "76.51% \n",
      "91.20% \n",
      "94.46% \n",
      "\n",
      "74.33% \n",
      "89.13% \n",
      "93.67% \n",
      "\n",
      "Fig. 5.  Roc and the Precision-Recall Curve. \n",
      "\n",
      "Table 2 \n",
      "Result of altering the number of hidden units. \n",
      "\n",
      "Method \n",
      "\n",
      "Prediction accuracy \n",
      "\n",
      "Loss \n",
      "\n",
      "Hybrid CNN-RNN, hidden units = 50 \n",
      "Hybrid CNN-RNN, hidden units = 100 \n",
      "Hybrid CNN-RNN, hidden units = 150 \n",
      "Hybrid CNN-RNN, hidden units = 200 \n",
      "\n",
      "92.32% \n",
      "93.57% \n",
      "94.21% \n",
      "92.53% \n",
      "\n",
      "4.73% \n",
      "4.72% \n",
      "4.43% \n",
      "4.68%\n",
      "\n",
      "Document 10 (Relevance Score: -0.21027667925276483):\n",
      "Table 1 presents the prediction accuracy of the proposed sin- \n",
      "gle frame regression CNN and Hybrid CNN-RNN technique imple- \n",
      "mented for predicting valence scores of subjects to developing a \n",
      "set  of  the  dataset.  Finally,  when  combining  the  information  and \n",
      "using the Hybrid CNN-RNN model with the ReLU, a signiﬁcant per- \n",
      "formance could be achived. \n",
      "\n",
      "Fig. 4.  Loss and the Prediction Accuracy for Hybrid CNN-RNN model. \n",
      "\n",
      "Table 1 \n",
      "Overall accuracy and mean accuracy for the different models. \n",
      "\n",
      "Method \n",
      "\n",
      "Overall accuracy  Mean class accuracy \n",
      "\n",
      "CNN \n",
      "CNN - RNN \n",
      "CNN - RNN + ReLU \n",
      "\n",
      "76.51% \n",
      "91.20% \n",
      "94.46% \n",
      "\n",
      "74.33% \n",
      "89.13% \n",
      "93.67% \n",
      "\n",
      "Fig. 5.  Roc and the Precision-Recall Curve. \n",
      "\n",
      "Table 2 \n",
      "Result of altering the number of hidden units. \n",
      "\n",
      "Method \n",
      "\n",
      "Prediction accuracy \n",
      "\n",
      "Loss \n",
      "\n",
      "Hybrid CNN-RNN, hidden units = 50 \n",
      "Hybrid CNN-RNN, hidden units = 100 \n",
      "Hybrid CNN-RNN, hidden units = 150 \n",
      "Hybrid CNN-RNN, hidden units = 200 \n",
      "\n",
      "92.32% \n",
      "93.57% \n",
      "94.21% \n",
      "92.53% \n",
      "\n",
      "4.73% \n",
      "4.72% \n",
      "4.43% \n",
      "4.68%\n",
      "\n",
      "LLM Answer: The accuracy of the document is not explicitly stated, as it refers to a specific study published on the topic (DOI: 10.1016/j.patrec.2018.04.010) and does not provide any information about the content or findings of the document itself.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the accuracies found in this document?\"\n",
    "results = vectorstore.similarity_search_with_relevance_scores(query, k=10)\n",
    "\n",
    "# Print retrieved documents and their scores for debugging\n",
    "for i, (doc, score) in enumerate(results):\n",
    "    print(f\"Document {i + 1} (Relevance Score: {score}):\\n{doc.}\\n\")\n",
    "\n",
    "# Run the QA chain if you want to see the LLM response\n",
    "result = qa_chain({\"query\": query})\n",
    "print(\"LLM Answer:\", result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 (Relevance Score: 1.485808253288269):\n",
      "Table 3 \n",
      "Result of altering the number of hidden layers. \n",
      "\n",
      "Method \n",
      "\n",
      "Prediction accuracy \n",
      "\n",
      "Loss \n",
      "\n",
      "HybridCNN-RNN, hidden layers = 4 \n",
      "HybridCNN-RNN, hidden layers = 5 \n",
      "Hybrid CNN-RNN, hidden layers = 6 \n",
      "\n",
      "94.57% \n",
      "94.73% \n",
      "94.91% \n",
      "\n",
      "4.28% \n",
      "4.22% \n",
      "3.98% \n",
      "\n",
      "Fig. 6.  Confusion matrices on JAFFE Datasets. \n",
      "\n",
      "Table 4 \n",
      "Proposed model versus other models performance comparison on \n",
      "JAFFE and MMI dataset. \n",
      "\n",
      "Method \n",
      "\n",
      "Accuracy of JAFFE \n",
      "\n",
      "Accuracy of MMI \n",
      "\n",
      "Zhang et al. [30] \n",
      "Khorrami et al. [31] \n",
      "Chernykh et al. [32] \n",
      "Fan et al. [33] \n",
      "Proposed model \n",
      "\n",
      "94.89% \n",
      "82.43% \n",
      "73% \n",
      "79.16% \n",
      "94.91% \n",
      "\n",
      "91.83% \n",
      "81.48% \n",
      "70.12% \n",
      "77.83% \n",
      "92.07% \n",
      "\n",
      "model. Hence, based on the experiments, the best results obtained \n",
      "by the 6 hidden layers”.\n",
      "\n",
      "Result 2 (Relevance Score: 1.485808253288269):\n",
      "Table 3 \n",
      "Result of altering the number of hidden layers. \n",
      "\n",
      "Method \n",
      "\n",
      "Prediction accuracy \n",
      "\n",
      "Loss \n",
      "\n",
      "HybridCNN-RNN, hidden layers = 4 \n",
      "HybridCNN-RNN, hidden layers = 5 \n",
      "Hybrid CNN-RNN, hidden layers = 6 \n",
      "\n",
      "94.57% \n",
      "94.73% \n",
      "94.91% \n",
      "\n",
      "4.28% \n",
      "4.22% \n",
      "3.98% \n",
      "\n",
      "Fig. 6.  Confusion matrices on JAFFE Datasets. \n",
      "\n",
      "Table 4 \n",
      "Proposed model versus other models performance comparison on \n",
      "JAFFE and MMI dataset. \n",
      "\n",
      "Method \n",
      "\n",
      "Accuracy of JAFFE \n",
      "\n",
      "Accuracy of MMI \n",
      "\n",
      "Zhang et al. [30] \n",
      "Khorrami et al. [31] \n",
      "Chernykh et al. [32] \n",
      "Fan et al. [33] \n",
      "Proposed model \n",
      "\n",
      "94.89% \n",
      "82.43% \n",
      "73% \n",
      "79.16% \n",
      "94.91% \n",
      "\n",
      "91.83% \n",
      "81.48% \n",
      "70.12% \n",
      "77.83% \n",
      "92.07% \n",
      "\n",
      "model. Hence, based on the experiments, the best results obtained \n",
      "by the 6 hidden layers”.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"whats the accuracies gotten by the proposed method ?\"\n",
    "\n",
    "results = vectorstore.similarity_search_with_score(query, k=2)\n",
    "\n",
    "for i, (document, score) in enumerate(results):\n",
    "    print(f\"Result {i + 1} (Relevance Score: {score}):\\n{document.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "UniqueConstraintError",
     "evalue": "Collection pdf_embeddings already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUniqueConstraintError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43mchroma_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m collection\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m      5\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m      6\u001b[0m     documents\u001b[38;5;241m=\u001b[39mchunks,\n\u001b[0;32m      7\u001b[0m     ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chunks))]\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents to collection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\chromadb\\api\\client.py:117\u001b[0m, in \u001b[0;36mClient.create_collection\u001b[1;34m(self, name, configuration, metadata, embedding_function, data_loader, get_or_create)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_collection\u001b[39m(\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     get_or_create: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    116\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[1;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Collection(\n\u001b[0;32m    126\u001b[0m         client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server,\n\u001b[0;32m    127\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    128\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[0;32m    129\u001b[0m         data_loader\u001b[38;5;241m=\u001b[39mdata_loader,\n\u001b[0;32m    130\u001b[0m     )\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\chromadb\\api\\segment.py:176\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[1;34m(self, name, configuration, metadata, get_or_create, tenant, database)\u001b[0m\n\u001b[0;32m    164\u001b[0m model \u001b[38;5;241m=\u001b[39m CollectionModel(\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m    166\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m     dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    174\u001b[0m )\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# TODO: Let sysdb create the collection directly from the model\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m coll, created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sysdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This is lazily populated on the first add\u001b[39;49;00m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# TODO: wrap sysdb call in try except and log error if it fails\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created:\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\chromadb\\db\\mixins\\sysdb.py:233\u001b[0m, in \u001b[0;36mSqlSysDB.create_collection\u001b[1;34m(self, id, name, configuration, metadata, dimension, get_or_create, tenant, database)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_collections(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mcollection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], tenant\u001b[38;5;241m=\u001b[39mtenant, database\u001b[38;5;241m=\u001b[39mdatabase\n\u001b[0;32m    229\u001b[0m             )[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    230\u001b[0m             \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    231\u001b[0m         )\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UniqueConstraintError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    235\u001b[0m collection \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m    237\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    244\u001b[0m )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtx() \u001b[38;5;28;01mas\u001b[39;00m cur:\n",
      "\u001b[1;31mUniqueConstraintError\u001b[0m: Collection pdf_embeddings already exists"
     ]
    }
   ],
   "source": [
    "collection_name = 'pdf_embeddings'\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    documents=chunks,\n",
    "    ids = [f'id_{i}' for i in range(len(chunks))]\n",
    ")\n",
    "\n",
    "print(f\"Added {len(chunks)} documents to collection '{collection_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results: {'ids': [['id_5', 'id_2', 'id_27', 'id_8', 'id_13']], 'distances': [[1.5282843112945557, 1.663107991218567, 1.6882652044296265, 1.704343557357788, 1.7343149185180664]], 'metadatas': [[None, None, None, None, None]], 'embeddings': None, 'documents': [['∗ Corresponding author. \\n\\nE-mail address: neha.juet@gmail.com (N. Jain). \\n\\nhttps://doi.org/10.1016/j.patrec.2018.04.010 \\n0167-8655/© 2018 Elsevier B.V. All rights reserved.', '© 2018 Elsevier B.V. All rights reserved. \\n\\n1. Introduction', '(2) \\n\\nFirstly  we  used  a  single  CNN  model  to  train  the  datasets.  At \\neach time trained a single image, the corresponding image passed \\nthrough the CNN model, the details of the model shown in Fig. 2 . \\nTwo fully-connected layers with 200 hidden units for the ap- \\nproximation of the valence label have been used. For the cost func- \\ntion the mean squared error has been used. For the network train- \\ning stochastic gradient descent while the batch size sets to 32 and \\nthe weight decay sets to 1E-4. Moreover, the learning rate at the \\nbeginning sets to 5e-3 which decrees by 0.01 every 20 epochs. \\n\\n\\x0c104 \\n\\nN. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106', '102 \\n\\nN. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106 \\n\\nposed an RNN to classify the facial emotion. The proposed model \\nexplores feature level fusion strategy and proves the moderate im- \\nprovement by this model. The other parts of the paper are orga- \\nnized as: next section delivers the related work in what we follow. \\nSection 3 presents the proposed network. The results and exper- \\niments are included in Section 4 . At the end, we have concluded \\nour observation in Section 5 . \\n\\n2. Related work', 'gression by mapping input successions to a grouping of concealed \\nstates, and furthermore mapping the covered up states to yields. \\nZhang et al. [30] “proposed a novel deep learning framework called \\nas a spatial-temporal recurrent neural network (STRNN) to unify']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n"
     ]
    }
   ],
   "source": [
    "# Example: Query the collection\n",
    "query_text = \"who wrote this paper? \"\n",
    "query_embedding = model1.encode([query_text]).tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding,\n",
    "    n_results=5,\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"Query results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformer' object has no attribute 'similarity_search_by_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Perform the similarity search in Chroma\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_by_vector\u001b[49m(query_embedding[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTop matches:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\torch\\nn\\modules\\module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SentenceTransformer' object has no attribute 'similarity_search_by_vector'"
     ]
    }
   ],
   "source": [
    "# Perform the similarity search in Chroma\n",
    "chroma_client.\n",
    "\n",
    "# Display results\n",
    "print(\"\\nTop matches:\")\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amedo\\AppData\\Local\\Temp\\ipykernel_7044\\2303327774.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a vector store for the document chunks\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m----> 4\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument embedded and stored in vector database\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:842\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    836\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[0;32m    837\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[0;32m    838\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    839\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    840\u001b[0m         )\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 842\u001b[0m     \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m(texts)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "# Create a vector store for the document chunks\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma.from_texts(chunks, embedding=embeddings)\n",
    "\n",
    "print(\"Document embedded and stored in vector database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create a vector store for the document chunks\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument embedded and stored in vector database\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:842\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    836\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[0;32m    837\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[0;32m    838\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    839\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    840\u001b[0m         )\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 842\u001b[0m     \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:671\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:497\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    495\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 497\u001b[0m     response \u001b[38;5;241m=\u001b[39m embed_with_retry(\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    499\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size],\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params,\n\u001b[0;32m    501\u001b[0m     )\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    503\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:120\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\openai\\resources\\embeddings.py:124\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    118\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    119\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\openai\\_base_client.py:1268\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1256\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1264\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1265\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1266\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1267\u001b[0m     )\n\u001b[1;32m-> 1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\openai\\_base_client.py:945\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    943\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 945\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\openai\\_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1033\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\openai\\_base_client.py:1083\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\openai\\_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1033\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\openai\\_base_client.py:1083\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Applications\\anaconda3\\envs\\pytorchs\\lib\\site-packages\\openai\\_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1048\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1049\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1052\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1053\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1057\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1058\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create a vector store for the document chunks\n",
    "vector_store = Chroma.from_texts(chunks, embedding=embeddings)\n",
    "\n",
    "print(\"Document embedded and stored in vector database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern Recognition Letters 115 (2018) 101–106\n",
      "Contents lists available at ScienceDirect\n",
      "Pattern  Recognition  Letters\n",
      "journal homepage: www.elsevier.com/locate/patrec\n",
      "Hybrid  deep  neural  networks  for  face  emotion  recognition\n",
      "Neha Jain a , ∗, Shishir Kumar a , Amit Kumar a , Pourya Shamsolmoali b , c ,\n",
      "Masoumeh Zareapoor b\n",
      "a\n",
      "Department of Computer Science & Engineering , Jaypee University of Engineering and Technology , Guna, India\n",
      "b\n",
      "Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai 200240, China\n",
      "c\n",
      "Advanced Scientiﬁc Computing Division, Euro-Mediterranean Centre on Climate Change (CMCC Foundation), Lecce, Italy\n",
      "a r t i c l e\n",
      "i n f o\n",
      "a b s t r a c t\n",
      "Article history:\n",
      "Available online 9 April 2018\n",
      "Keywords:\n",
      "Emotion recognition\n",
      "Deep learning\n",
      "Recurrent neural networks\n",
      "Convolutional Neural Networks\n",
      "Hybrid CNN-RNN\n",
      "Deep Neural Networks (DNNs) outperform traditional models in numerous optical recognition missions\n",
      "containing Facial Expression Recognition (FER) which is an imperative process in next-generation Human-\n",
      "Machine Interaction (HMI) for clinical practice and behavioral description. Existing FER methods do not\n",
      "have high accuracy and are not suﬃcient practical in real-time applications. This work proposes a Hybrid\n",
      "Convolution-Recurrent Neural Network method for FER in Images. The proposed network architecture\n",
      "consists of Convolution layers followed by Recurrent Neural Network (RNN) which the combined model\n",
      "extracts the relations within facial images and by using the recurrent network the temporal dependencies\n",
      "which exist in the images can be considered during the classiﬁcation. The proposed hybrid model is eval-\n",
      "uated based on two public datasets and Promising experimental results have been obtained as compared\n",
      "to the state-of-the-art methods.\n",
      "© 2018 Elsevier B.V. All rights reserved.\n",
      "1. Introduction\n",
      "Facial and emotional expressions are the most signiﬁcant non-\n",
      "verbal ways for expressing internal emotions and intentions.“Facial\n",
      "Action Coding system (FACS) is a useful structure that classiﬁes the\n",
      "human facial actions by their advent on the face using Action Units\n",
      "(AU). An AU is one of 46 minor elements of visible facial motion or\n",
      "its relatedform changes.Facial expressions have worldwide mean-\n",
      "ing, and these emotions have been accepted for tens and even hun-\n",
      "dreds of years and it was the main reason for us to select facial ex-\n",
      "pressions for the research”. These days, interest in emotion recog-\n",
      "nition (ER) has skyrocketed, while it stayed as single main diﬃcul-\n",
      "ties in the area of human-computer interaction. The cornerstone\n",
      "of  the  most  relevant  research  is  to  build  a  reliable  conversation\n",
      "and communication among human and computer (machine). The\n",
      "importance of ER methods can be achieved by either “make hu-\n",
      "mans to understand computer/machine accurately and conversely”.\n",
      "Facial  Expression  Recognition  (FER)  is  a  challenging  task  in  ma-\n",
      "chine learning with a wide-ranging of applications in healthcare,\n",
      "human-computer interaction, and gaming. Emotion recognition is\n",
      "challenging due to several input modalities, have a signiﬁcant role\n",
      "in understanding it. “The mission of recognizing of the emotions is\n",
      "mostly  diﬃcult  dueto  two  main  reasons:  1)  There  is  not  largely\n",
      "∗ Corresponding author.\n",
      "E-mail address: neha.juet@gmail.com (N. Jain).\n",
      "https://doi.org/10.1016/j.patrec.2018.04.010\n",
      "0167-8655/© 2018 Elsevier B.V. All rights reserved.\n",
      "available  database  of  training  images  and  2)  classifying  emotion\n",
      "could  not  be  simplebased  on  whether  the  input  image  is  static\n",
      "or aevolution frame into a facial expression. The ﬁnaldiﬃculty is\n",
      "mostly for the real-time detection while facial expressions differ-\n",
      "enthusiastically”.  Ekman  et  al.  [1]  counted  six  expressions  (sur-\n",
      "prise, fear, happiness, anger, disgust, and sadness) as main emo-\n",
      "tional expressions that are common among human beings. Mostly\n",
      "the big overlap between the emotion classes makes the classiﬁca-\n",
      "tion task very diﬃcult. This paper proposed a deep learning tech-\n",
      "nique in the context of emotional recognition, in order to classify\n",
      "emotion labels from the images. Too many methods and research\n",
      "has been developed in this regards, however, most current works\n",
      "are appeared focusing on hand-engineered features [2,3] . Now a\n",
      "day’s due to quantity and variety of datasets, deep learning is be-\n",
      "coming  as  mainstream  techniques  in  all  computer  visions  tasks\n",
      "[4,5] . Conventional convolutional neural systems have a notewor-\n",
      "thy constraint that they simply handle spatial image. The essential\n",
      "commitment of this work is to display the spatio-worldly develop-\n",
      "ment of outward appearances of a man in the Images utilizing a\n",
      "“Recurrent Neural Network (RNN) which embedded with a Convo-\n",
      "lutional Neural Network (CNN) in a form of CNN–RNN design”. We\n",
      "additionally introduce a neural system based element level com-\n",
      "bination procedure to join diverse modalities for the last emotion\n",
      "forecast. The pioneering works in emotion recognition based deep\n",
      "learning [6,7] has achieved the state-of-the-art. The cornerstone of\n",
      "these proposed models [6,8] is an average-based aggregation for\n",
      "visual  features.  A  little  distinguish  from  current  works,  we  pro-\n",
      "102\n",
      "N. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106\n",
      "posed an RNN to classify the facial emotion. The proposed model\n",
      "explores feature level fusion strategy and proves the moderate im-\n",
      "provement by this model. The other parts of the paper are orga-\n",
      "nized as: next section delivers the related work in what we follow.\n",
      "Section 3 presents the proposed network. The results and exper-\n",
      "iments are included in Section 4 . At the end, we have concluded\n",
      "our observation in Section 5 .\n",
      "2. Related work\n",
      "“Generally, research works in this area have been focused on\n",
      "identifying human emotion in the base of video footage or based\n",
      "on audiovisual records (mixing speech recognition and video tech-\n",
      "niques).  Several  papers  pursue  to  identify  and  match  faces  [20] ,\n",
      "nevertheless  most  works  did  not  use  deep  learning  to  extract\n",
      "emotions from images”. Customarily, calculations for mechanized\n",
      "outward appearance acknowledgment comprises of three primary\n",
      "modules,  viz.  enlistment,  highlight  extraction,  and  arrangement.\n",
      "Point by point study of various approaches in every one of these\n",
      "means can be found in [9] . “Customary calculations for full of feel-\n",
      "ing  registering  from  faces  utilize  designed  highlights  for  exam-\n",
      "ple,  Histogram  of  Oriented  Gradients  [11] ,  Local  Binary  Patterns\n",
      "[10] and facial historic points [12] ”. Since the greater parts of these\n",
      "highlights are hand-created for their particular use of acknowledg-\n",
      "ment, so the generalization in the particular situation is necessary,\n",
      "such as, high variability in lighting, subjects ethnicity, visual de-\n",
      "termination, and so on. Interestingly, the powerful methodologies\n",
      "for accomplishing a great acknowledgment for series of marking\n",
      "errand are alluded to separate the transient relations of edges in\n",
      "an arrangement. Separating these transient relations have been ex-\n",
      "amined utilizing customary techniques before. Cases of these en-\n",
      "deavors  are  Concealed  Markov  Models  [13,14,47,48]  “(which  join\n",
      "the  data  and  then  apply  division  on  recordings),  Spatio  Tempo-\n",
      "ral  Shrouded  Markov  Models  by  combing  S-HMM  and  T-HMM\n",
      "[15] , Dynamic Bayesian Networks” [16] is related to multi-tactile\n",
      "data combination paradigm, Bayesian transient models to catch the\n",
      "dynamic  outward  appearance  progress,  and  Conditional  Irregular\n",
      "Fields (CRFs) [17,18] and their augmentations. Recently, \"Convolu-\n",
      "tional  Neural  Networks\"  (CNN)  has  turned  into  the  most  main-\n",
      "stream approach in the deep learning techniques. AlexNet [19] de-\n",
      "pends on the conventional layered engineering which comprises of\n",
      "a few convolution layers, max-pooling layers and Rectiﬁed Linear\n",
      "Units (ReLUs). Szegedy et al. [20] presented GoogLeNet which is\n",
      "made out of numerous \"Beginning” layers. Commencement applies\n",
      "a few convolutions on the include outline distinctive scales. Molla-\n",
      "hosseini et al. [21] have utilized the Inception layer for the under-\n",
      "taking of outward appearance acknowledgment and accomplished\n",
      "best in class comes about. Following the accomplishment of Incep-\n",
      "tion layers, a few varieties of them have been proposed [22] . “RNNs\n",
      "recently have greatly succeeded in handling sequential data such\n",
      "as speech recognition [23] , natural language processing [24,25] , ac-\n",
      "tion recognition [26] , and so on. Then RNN is additionally has been\n",
      "improved  to  treat  the  images  [27]  by  scanning  the  parts  of  im-\n",
      "ages into sequences in certain directions. Due to the capability of\n",
      "recollecting information about the past inputs, RNN has the abil-\n",
      "ity to learn relative dependencies with images, which is advanta-\n",
      "geous in comparison with CNN. The reason is CNN may fail to learn\n",
      "the overall dependencies because of the locality of convolution and\n",
      "pooling  layers.  Therefore,  RNN  is  generally  combined  with  CNN\n",
      "in order to achieve better achievement in image processing tasks\n",
      "such as image recognition [28] and segmentation [29] ”. Conven-\n",
      "tional Recurrent Neural Networks  (RNNs) can learn ﬂeeting pro-\n",
      "gression by mapping input successions to a grouping of concealed\n",
      "states, and furthermore mapping the covered up states to yields.\n",
      "Zhang et al. [30] “proposed a novel deep learning framework called\n",
      "as a spatial-temporal recurrent neural network (STRNN) to unify\n",
      "the learning of two different signal sources into a spatial-temporal\n",
      "dependency  model”.  Khorrami  et  al.  in  [31,45,46] ,  developed  a\n",
      "method which used the CNN and RNN in order to perform emo-\n",
      "tion recognition on video data. Chernykh et al. [32] and Fan et al.\n",
      "[33] proposed CNN + RNN models for the video and speech recog-\n",
      "nition. In spite of the fact that RNNs have demonstrated promis-\n",
      "ing execution on different assignments, it is diﬃcult for them to\n",
      "learn  long  haul  successions.  This  is  mostly  the  result  of  vanish-\n",
      "ing/detonating slopes issue [34] that can be understood by having\n",
      "a memory for recalling and overlooking the past states. “Xie and\n",
      "Hu [42] presented a new CNNmodel that used convolutional mod-\n",
      "ules. tominimize redundancy of same features learned, considers\n",
      "communal information among ﬁlters of the same layer, and offers\n",
      "the top set of features for the next layer.A distinguishedapplication\n",
      "of a CNN to real-time detection of emotions from facial expressions\n",
      "is by Oullet [43] . Theymade a game, while a CNN was applied to\n",
      "a video stream to grab the subject’s facial expressions, performing\n",
      "as a controller for the game. This work established the possibil-\n",
      "ity of executing a CNN in real-time by means of a running-average\n",
      "of the perceived emotions from the input stream, decreasing the\n",
      "special effects of variation and noise. A latest development by Levi\n",
      "et al. [44] illustrated important upgrading in facial emotion recog-\n",
      "nition using a CNN. They listed two main drawbacks: 1) the small\n",
      "amount of available data for training deep CNNs and 2) appearance\n",
      "dissimilarity generally affected by dissimilarities in illumination”.\n",
      "Distinct from other work including video and RNN strategies,\n",
      "[35] ,  in  this  paper  we  don’t  utilize  LSTMs.  However,  we  utilize\n",
      "IRNNs  [36]  that  is  made  out  of  amended  straight  units  (ReLUs)\n",
      "what’s more; utilize a unique introduction system in view of scaled\n",
      "varieties  of  the  character  grid.  These  components  of  IRNNs  are\n",
      "gone  for  giving  a  substantially  less  diﬃcult  system  to  managing\n",
      "with the vanishing and detonating inclination issue thought about\n",
      "to  the  more  perplexing  LSTM  system.  Late  work  has  contrasted\n",
      "IRNNs and LSTMs and found that IRNNs can yield equivalent out-\n",
      "comes in a few errands, including issues which include long haul\n",
      "conditions” [36] . We give point by point details of the CNN and\n",
      "the RNN structure the in next Section. Moreover, we concatenated\n",
      "the CNN highlights to a permanent distance feature vectors and\n",
      "furthermore, trained on SVM.\n",
      "3. Proposed model\n",
      "The opposition dataset has only a single emotion label for each\n",
      "picture and do not have relation to each casing. This presents a\n",
      "great deal of commotion if the picture labels are utilized as focuses\n",
      "on preparing a CNN on the singular image. Our visual highlights\n",
      "are in this way given by a CNN prepared on a mix of two extra\n",
      "emotion datasets of static pictures. In addition, utilizing extra in-\n",
      "formation covers a bigger assortment of age and character rather\n",
      "than  the  test  information  where  a  similar  performing  artist/on-\n",
      "screen character might show up in numerous clips. For the CNN\n",
      "training we used two large emotion datasets, MMI Facial Expres-\n",
      "sion Database (TFD) [37] it consists more than 2900 images of 75\n",
      "subjects and “Japanese Female Facial Expression (JAFFE) Database\n",
      "[38] containing 213 pictures, which have seven basic expressions:\n",
      "angry, sad, surprise, happy, disgust, fear, and neutral”.\n",
      "For  the  preprocessing,  we  represent  ﬂuctuating  lighting  con-\n",
      "ditions  (speciﬁcally,  crosswise  over  datasets)  we  connected  his-\n",
      "togram evening out. We utilized the adjusted appearances gave by\n",
      "the coordinators to remove highlights from the CNN. “The arrange-\n",
      "ment  includes  a  joined  facial  key  point’s  location  and  following\n",
      "methodology clariﬁed in [39] . Extraordinary confront location, as\n",
      "well as arrangement procedures, have been utilized for MMI Facial\n",
      "Expression and the JAFFE Datasets”. Keeping in mind the end goal\n",
      "to be ready to use the extra datasets, we re-adjusted all datasets\n",
      "to JAFFE utilizing the accompanying method:\n",
      "N. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106\n",
      "103\n",
      "Eq. (3) illustrates the Adam update and its combination with\n",
      "the momentum update.\n",
      "m t = β1 m t−1 + (1 − β1 ) ∇ X t−1\n",
      "v t = β2 v t−1 + (1 − β2 ) ∇ ( X t−1 ) 2\n",
      "X t = X t−1 − a m t √\n",
      "v t + ε\n",
      "(3)\n",
      "β1, β2 ∈ [0,1] and ɛ are hyperparameters, m t is the momentum\n",
      "vector with t iteration, v t  is the velocity vector, and the learning\n",
      "rate of α. Adam is the actual update algorithm due to information\n",
      "usage for the primary and the secondary moments of the gradient.\n",
      "The CNN is used primarily for feature extraction and we have\n",
      "just  utilized  the  extra  dataset  for  the  training.  Accordingly,  we\n",
      "hunt  down  a  model  that  have  better  communalize  to  different\n",
      "datasets. Profound models are known to learn portrayals to have\n",
      "better communalize to different datasets. By the way, it has been\n",
      "found out that the deep structure rapidly over-ﬁtted, and commu-\n",
      "nalize severely to the test dataset. This could be because of the\n",
      "generally little measure of marked information accessible for the\n",
      "emotion detection tasks. Consequently, “we build different connec-\n",
      "tions between 6 layers which seem to have decent tended to the\n",
      "over-ﬁtting issues. At the end, we expanded the ﬁlter size from 3\n",
      "to 5 and the numbers of channels are 8-16-32-64-128-256. For the\n",
      "experimentations data augmentation has been used” ((horizontal,\n",
      "vertical and rotation ﬂipping with 0.25 probability), and dropout is\n",
      "used (with the rate of 0.5).\n",
      "RNNs are a form of neural network that converts the order of\n",
      "inputs into a series of outputs. In separately time step t , an un-\n",
      "known parameter h t  is calculated according to the unknown pa-\n",
      "rameter at time t − 1 and the input x t at time t\n",
      "h t = σ ( W in x t + W rec h t−1 )\n",
      "(4)\n",
      "While “W\n",
      "in  is the weight of input matrix, W rec  is the matrix\n",
      "of recurrent and σ is the hidden activation function. Respectively\n",
      "time step similarly calculates the outputs, relying on the existing\n",
      "hidden state”:\n",
      "y t = f ( W out h t )\n",
      "(5)\n",
      "While W out is the result weighted parameters and f is the ac-\n",
      "tivation function of the output. An instance of an RNN in which\n",
      "merely the last phase creates the output which illustrated in Fig. 3 .\n",
      "“An  RNN  model  has  been  used,  that  previously  discussed  by\n",
      "using  rectiﬁed  linear  units  (ReLUs)  and  recurrent  matrix,  which\n",
      "is  adjusted  with  scaled  deviations  of  the  distinctiveness  matrix”\n",
      "[42] . The distinctiveness initialization model conﬁrms good gradi-\n",
      "ent movement at the commencement of training and it consents to\n",
      "train it on moderately extensive orders. The RNN ha been trained\n",
      "to categorize the images by inserting the extracted features of each\n",
      "image from the CNN serially network and ﬁnally using the Soft-\n",
      "max for the prediction. In the implementation the Gradient clip-\n",
      "ping rated to 1.0 and a batch size set to 32. We tested the model\n",
      "by using several layers of the CNN as input features and picked the\n",
      "output of every third convolutional layer right after max pooling,\n",
      "as this achieved the highest result on validation data.\n",
      "Fig. 1.  Sample of JAFFE dataset for ﬁve type of emotion (Ang, Sad, Fea, Hap, Sur).\n",
      "1. We distinguished ﬁve facial key focuses for all pictures in the\n",
      "JAFFE and MMI preparing set utilizing the convolutional neural\n",
      "system course strategy in [40] .\n",
      "2. for every dataset, the mean shape have been processed by av-\n",
      "eraging the directions of main focuses.\n",
      "3. The datasets have been mapped by utilizing a closeness change\n",
      "among  the  mean  shapes.  By  processing  one  change  for  each\n",
      "dataset the nose, eyes, and mouth is generally in a similar area\n",
      "holding a slight measure of variety. We included an uproarious\n",
      "fringe for MMI and JAFFE-faces as appearances were edited all\n",
      "the more ﬁrmly contrasted with JAFFE.\n",
      "4. JAFFE-faces  approval  test  sets  were  mapped  to  utilize  the\n",
      "change construed on the preparation set.\n",
      "Additionally, dataset standardization has been performed by us-\n",
      "ing the standard deviation and mean picture from the consolidated\n",
      "JAFFE and MMI (JAFFE + MMI). Fig. 1 represents the samples face\n",
      "emotion data. For the implementation and the evaluation of the\n",
      "proposed model the 70% of each dataset used for training and the\n",
      "rest 30% for testing.\n",
      "3.1. Convolution neural network architecture\n",
      "Emotion  Recognition  data  comes  in  various  sizes  and  resolu-\n",
      "tions, so we try to propose a model which can handle any type\n",
      "of input. In our approach, “we considered a class of networks with\n",
      "6 convolutional layers plus 2 fully connected layers”, each with a\n",
      "ReLu activation function, and dropout for training.\n",
      "Plus 2 fully connected layers”, each with a ReLu activation func-\n",
      "tion, and dropout for training. Furthermore, we performed regular-\n",
      "ization for each weight matrix W that limits the size of the weights\n",
      "at individual layer by adding a term to the loss equal to some ﬁxed\n",
      "hyperparameter. We explain these in Eq. (1) , where x be the output\n",
      "of a particular neuron in the network and p the dropout possibility.\n",
      "(cid:2)\n",
      "ReLu (x ) = max (0 , x )\n",
      "Dropout (x, p) =\n",
      "Reg(w ) = λ|| w || 2\n",
      "2\n",
      "x, with prob . p\n",
      "x, with prob . 1 − p\n",
      "(1)\n",
      "3.2. Regression CNN\n",
      "Combinations of two deep learning initializer algorithms have\n",
      "been  used  to  perform  parameter  updates  based  on  the  gradi-\n",
      "ent of the loss function called as Momentum and Adam [41,42] .\n",
      "Eq. (2) describes this update, where X t is parameter matrix at iter-\n",
      "ation t. v t is the velocity vector at iteration t, and α is the rate of\n",
      "learning.\n",
      "v t = γ v t−1 − a ∇ X t−1\n",
      "X t = X t−1 + v t\n",
      "(2)\n",
      "Firstly  we  used  a  single  CNN  model  to  train  the  datasets.  At\n",
      "each time trained a single image, the corresponding image passed\n",
      "through the CNN model, the details of the model shown in Fig. 2 .\n",
      "Two fully-connected layers with 200 hidden units for the ap-\n",
      "proximation of the valence label have been used. For the cost func-\n",
      "tion the mean squared error has been used. For the network train-\n",
      "ing stochastic gradient descent while the batch size sets to 32 and\n",
      "the weight decay sets to 1E-4. Moreover, the learning rate at the\n",
      "beginning sets to 5e-3 which decrees by 0.01 every 20 epochs.\n",
      "104\n",
      "N. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106\n",
      "Fig. 2.  CNN Architecture, the network contains six convolutional layers containing\n",
      "8, 16, 32, 64, 128, and 256 ﬁlters; each of size 5 × 5 and 3 × 3 followed by ReLU ac-\n",
      "tivation functions. 3 × 3 max-pooling layers added just after every ﬁrst ﬁve convolu-\n",
      "tional layers and average pooling at the last convolution layer. Every convolutional\n",
      "layer has two fully-connected layers and 200 hidden units.\n",
      "Fig. 3.  Hybrid CNN-RNN Network Architecture.\n",
      "3.3. Combining with recurrent neural networks (RNNs)\n",
      "In the proposed model like the model which presented by [31] ,\n",
      "we propose to combine the sequential information by using RNN\n",
      "to spread information. The CNN model used for feature extraction\n",
      "to ﬁx all of its parameters and to eliminate the regression layer.\n",
      "For the processing, when the image passed to the network, 200-\n",
      "dimensional vectors will be extracted from the fully-connected lay-\n",
      "ers. For the assumed time t, we take P frames from the past (i.e.\n",
      "[ t − P, t ]). Then passes every frame from time t − P to t to the CNN\n",
      "and extract P vectors fully for each image. Each and every vector\n",
      "goes through a node of the RNN model. Then every node of the\n",
      "RNN returns some results of valence label. The overall proposed\n",
      "method illustrated in Fig. 3 . The mean squared error has been used\n",
      "for the cost function while optimizing.\n",
      "4. Experiment and evaluation\n",
      "For the data preprocessing, we initially identify the face in ev-\n",
      "ery outline utilizing face and point of interest ﬁnder. Then map the\n",
      "distinguished landmark points to characterized pixel areas in a re-\n",
      "quest to guarantee correspondence concerning outlines. After the\n",
      "normalization the nose, mouth and nose organizes, while process-\n",
      "ing each face image through the CCN mean subtraction and con-\n",
      "trast normalization applied. We tested the proposed models on a\n",
      "normal PC with Intel(R) Core(TM) i7-8700 K and 24 GB of RAM.\n",
      "4.1. Compare the CNN with hybrid CNN-RNN\n",
      "Fig. 4 shows the loss and the prediction accuracy of the Hybrid\n",
      "CNN-RNN model for training vas validation for one set of the Im-\n",
      "ages. These charts clearly illustrate the smooth performance of the\n",
      "proposed model.\n",
      "Table 1 presents the prediction accuracy of the proposed sin-\n",
      "gle frame regression CNN and Hybrid CNN-RNN technique imple-\n",
      "mented for predicting valence scores of subjects to developing a\n",
      "set  of  the  dataset.  Finally,  when  combining  the  information  and\n",
      "using the Hybrid CNN-RNN model with the ReLU, a signiﬁcant per-\n",
      "formance could be achived.\n",
      "Fig. 4.  Loss and the Prediction Accuracy for Hybrid CNN-RNN model.\n",
      "Table 1\n",
      "Overall accuracy and mean accuracy for the different models.\n",
      "Method\n",
      "Overall accuracy  Mean class accuracy\n",
      "CNN\n",
      "CNN - RNN\n",
      "CNN - RNN + ReLU\n",
      "76.51%\n",
      "91.20%\n",
      "94.46%\n",
      "74.33%\n",
      "89.13%\n",
      "93.67%\n",
      "Fig. 5.  Roc and the Precision-Recall Curve.\n",
      "Table 2\n",
      "Result of altering the number of hidden units.\n",
      "Method\n",
      "Prediction accuracy\n",
      "Loss\n",
      "Hybrid CNN-RNN, hidden units = 50\n",
      "Hybrid CNN-RNN, hidden units = 100\n",
      "Hybrid CNN-RNN, hidden units = 150\n",
      "Hybrid CNN-RNN, hidden units = 200\n",
      "92.32%\n",
      "93.57%\n",
      "94.21%\n",
      "92.53%\n",
      "4.73%\n",
      "4.72%\n",
      "4.43%\n",
      "4.68%\n",
      "Fig. 5 displays the Roc curve and the Precision-Recall curve of\n",
      "the proposed hybrid model. As it is visible the proposed model has\n",
      "the ability to with the  least number of  errors  used for the face\n",
      "emotion detection.\n",
      "We evaluated the special effects of two hyperparameters in the\n",
      "results of Hybrid proposed model, namely the number of hidden\n",
      "units  and  the  number  of  hidden  layers.  Table  2  concluded  that,\n",
      "the best result can achieve with 150 hidden units and in the other\n",
      "cases rather than improvement in the performance resulted in de-\n",
      "creases. Table 3 “shows that increasing the number of hidden lay-\n",
      "ers resulted to improve the overall performance of the proposed\n",
      "N. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106\n",
      "105\n",
      "Table 3\n",
      "Result of altering the number of hidden layers.\n",
      "Method\n",
      "Prediction accuracy\n",
      "Loss\n",
      "HybridCNN-RNN, hidden layers = 4\n",
      "HybridCNN-RNN, hidden layers = 5\n",
      "Hybrid CNN-RNN, hidden layers = 6\n",
      "94.57%\n",
      "94.73%\n",
      "94.91%\n",
      "4.28%\n",
      "4.22%\n",
      "3.98%\n",
      "Fig. 6.  Confusion matrices on JAFFE Datasets.\n",
      "Table 4\n",
      "Proposed model versus other models performance comparison on\n",
      "JAFFE and MMI dataset.\n",
      "Method\n",
      "Accuracy of JAFFE\n",
      "Accuracy of MMI\n",
      "Zhang et al. [30]\n",
      "Khorrami et al. [31]\n",
      "Chernykh et al. [32]\n",
      "Fan et al. [33]\n",
      "Proposed model\n",
      "94.89%\n",
      "82.43%\n",
      "73%\n",
      "79.16%\n",
      "94.91%\n",
      "91.83%\n",
      "81.48%\n",
      "70.12%\n",
      "77.83%\n",
      "92.07%\n",
      "model. Hence, based on the experiments, the best results obtained\n",
      "by the 6 hidden layers”.\n",
      "The confusion matrices of CNN and Hybrid CNN-RNN models\n",
      "on the testing sets are presented in Fig. 6 . Hybrid CNN-RNN model\n",
      "could achieve an accuracy of 94.72%, while a single CNN can reach\n",
      "only to 71.42%. The combined model not only increases the overall\n",
      "accuracy of the proposed CNN model but also it reduces the false\n",
      "detection of the model. As it is clearly visible in the Fig. 6 the best\n",
      "detection are for the Ang, Neu, and Sur emotions.\n",
      "Table 4 indications the performance of proposed Hybrid CNN-\n",
      "RNN model in comparison with other approaches evaluated on the\n",
      "JAFFE and MMI datasets. The proposed CNN-RNN model achieved\n",
      "equal or greater performance as compared to the four other state-\n",
      "of-the-art methods [30–33] .\n",
      "4.2. Comparison of the proposed model with other approaches\n",
      "Our  model  has  slightly  better  performance  than  the  model\n",
      "which proposed by Zhang et al. [30] , While, the other models have\n",
      "the lower performance in comparison with the proposed model.\n",
      "5. Conclusion\n",
      "In  this  paper,  a  model  has  been  proposed  for  face  emotion\n",
      "recognition. We proposed a hybrid deep CNN and RNN model. In\n",
      "addition,  the  proposed  model  evaluated  under  different  circum-\n",
      "stances  and  hyper  parameters  to  properly  tuning  the  proposed\n",
      "model. Particularly, it has been found that the combination of the\n",
      "two types of neural networks (CNN-RNN) cloud signiﬁcantly im-\n",
      "prove the overall result of detection, which veriﬁed the eﬃciency\n",
      "of the proposed model.\n",
      "References\n",
      "[1] P. Ekman , W.V. Friesen , Constants across cultures in the face and emotion, J.\n",
      "Pers. Soc. Psychol. 17 (2) (1971) 124 .\n",
      "[2] S.E. Kahou , P. Froumenty , C. Pal , Facial expression analysis based on high di-\n",
      "mensional binary features, ECCV Workshop on Computer Vision with Local Bi-\n",
      "nary Patterns Variants, 2014 .\n",
      "[3] C. Shan , S. Gong , P.W. McOwan , Facial expression recognition based on local\n",
      "binary patterns: a comprehensive study, Image Vis. Comput. 27 (6) (May 2009)\n",
      "803–816 .\n",
      "[4] N. Kalchbrenner, E. Grefenstette, and P. Blunsom. A convolutional neural net-\n",
      "work for modelling sentences. arXiv: 1404.2188 , 2014.\n",
      "[5] A. Krizhevsky , I. Sutskever , G.E. Hinton , Imagenet classiﬁcation with deep con-\n",
      "volutional neural networks, Adv. Neural Inf. Process. Syst. (2012) 1097–1105 .\n",
      "[6] S.E. Kahou , C. Pal , X. Bouthillier , P. Froumenty , C. Gulcehre , et al. , Combining\n",
      "modality speciﬁc deep neural networks for emotion recognition in video, In-\n",
      "ternational Conference on Multimodal Interaction, ICMI ’13, 2013 .\n",
      "[7] M. Liu , R. Wang , S. Li , S. Shan , Z. Huang , X. Chen , Combining multiple kernel\n",
      "methods on riemannian manifold for emotion recognition in the wild, in: In-\n",
      "ternational Conference on Multimodal Interaction, ICMI ’14, 2014, pp. 494–501 .\n",
      "[8] S.E. Kahou , X. Bouthillier , P. Lamblin , C. Gulcehre , V. Michalski , et al. , Emon-\n",
      "ets: multimodal deep learning approaches for emotion recognition in video, J.\n",
      "Multimodal User Interfaces (2015) 1–13 .\n",
      "[9] E. Sariyanidi , H. Gunes , A. Cavallaro , Automatic analysis of facial affect: a sur-\n",
      "vey of registration, representation, and recognition, IEEE Trans. Pattern Anal.\n",
      "Mach. Intell. 37 (6) (2015) 1113–1133 .\n",
      "[10] C. Shan , S. Gong , P.W. McOwan , Facial expression recognition based on lo-\n",
      "cal binary patterns: a comprehensive study, Image Vis. Comput. 27 (6) (2009)\n",
      "803–816 .\n",
      "[11] N. Dalal , B. Triggs , Histograms of oriented gradients for human detection, in:\n",
      "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer\n",
      "Society Conference on, 1, IEEE, 2005, pp. 886–893 .\n",
      "[12] T.F. Cootes , G.J. Edwards , C.J. Taylor , et al. , Active appearance models, IEEE\n",
      "Trans. Pattern Anal. Mach. Intell. 23 (6) (2001) 6 81–6 85 .\n",
      "[13] M. Yeasin , B. Bullot , R. Sharma , Recognition of facial expressions and measure-\n",
      "ment of levels of interest from video, Multimedia, IEEE Trans. 8 (3) (2006)\n",
      "500–508 .\n",
      "[14] Y. Zhu , L.C. De Silva , C.C. Ko , Using moment invariants and hmm in facial ex-\n",
      "pression recognition, Pattern Recognit. Lett. 23 (1) (2002) 83–91 .\n",
      "[15] Y. Sun , X. Chen , M. Rosato , L. Yin , Tracking vertex ﬂow and model adaptation\n",
      "for three-dimensional spatiotemporal face analysis, Syst. Man Cybern. Part A\n",
      "40 (3) (2010) 461–474 .\n",
      "[16] N. Sebe , M.S. Lew , Y. Sun , I. Cohen , T. Gevers , T.S. Huang , Authentic facial ex-\n",
      "pression analysis, Image Vis. Comput. 25 (12) (2007) 1856–1863 .\n",
      "[17] B. Hasani , M.M. Arzani , M. Fathy , K. Raahemifar , Facial expression recognition\n",
      "with discriminatory graphical models, in: 2016 2nd International Conference\n",
      "of Signal Processing and Intelligent Systems (ICSPIS), Dec 2016, pp. 1–7 .\n",
      "[18] B. Hasani and M.H. Mahoor. Spatio-temporal facial expression recognition us-\n",
      "ing convolutional neural networks and conditional random ﬁelds. arXiv: 1703.\n",
      "06995 , 2017.\n",
      "[19] A. Krizhevsky , I. Sutskever , G.E. Hinton , Imagenet classiﬁcation with deep con-\n",
      "volutional neural networks, Adv. Neural Inf. Process. Syst. (2012) 1097–1105 .\n",
      "[20] C. Szegedy , W. Liu , Y. Jia , P. Sermanet , S. Reed , D. Anguelov , D. Erhan , V. Van-\n",
      "houcke , A. Rabinovich , Going deeper with convolutions, in: Proceedings of the\n",
      "IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1–9 .\n",
      "[21] A. Mollahosseini , B. Hasani , M.J. Salvador , H. Abdollahi , D. Chan , M.H. Mahoor ,\n",
      "Facial expression recognition from world wild web, In The IEEE Conference on\n",
      "Computer Vision and Pattern Recognition (CVPR) Workshops, June 2016 .\n",
      "[22] S. Ioffe and C. Szegedy. Batch normalization: accelerating deep network train-\n",
      "ing by reducing internal covariate shift. arXiv: 1502.03167 , 2015.\n",
      "[23] A . Graves , A . r. Mohamed , G. Hinton , Speech recognition with deep recurrent\n",
      "neural networks, in: Proc. IEEE International Conference on Acoustics, Speech\n",
      "and Signal Processing, 2013, pp. 6645–6649 .\n",
      "106\n",
      "N. Jain et al. / Pattern Recognition Letters 115 (2018) 101–106\n",
      "[24] A. Graves , N. Jaitly , Towards end-to-end speech recognition with recurrent\n",
      "neural networks, in: Proc. International Conference on Machine Learning, 2014,\n",
      "pp. 1764–1772 .\n",
      "[25] T.  Mikolov ,  M.  Karaﬁ ´at ,  L.  Burget ,  J.  Cernock  ‘y ,  S.  Khudanpur , Recurrent\n",
      "neural  network  based  language  model,  in:  Proc.  INTERSPEECH,  2,  2010,\n",
      "pp. 1045–1048 .\n",
      "[26] A. Sanin , C. Sanderson , M.T. Harandi , B.C. Lovell , Spatiotemporal covariance de-\n",
      "scriptors for action and gesture recognition, IEEE Workshop on Applications of\n",
      "Computer Vision, 2013 .\n",
      "[27] S. Jain , C. Hu , J.K. Aggarwal , Facial expression recognition with temporal mod-\n",
      "eling of shapes, in: Proc. IEEE International Conference on Computer Vision\n",
      "Workshops, 2011, pp. 1642–1649 .\n",
      "[28] F. Visin, K. Kastner, K. Cho, M. Matteucci, et al., Renet: A recurrent neural net-\n",
      "work based alternative to convolutional networks. arXiv: 1505.00393 , 2015\n",
      "[29] F. Visin, K. Kastner, A. Courville, Y. Bengio, et al., ReSeg: a recurrent neural\n",
      "network for object segmentation. arXiv: 1511.07053 , 2015\n",
      "[30] T. Zhang, W. Zheng, Z. Cui, Y. Zong, Y. Li, Spatial-temporal recurrent neu-\n",
      "ral  network  for  emotion  recognition,  IEEE  Trans.  Cybern.  (99)  (2018)  1–9\n",
      "arXiv: 1705.04515 .\n",
      "[31] P. Khorrami , T.L. Paine , K. Brady , C. Dagli , T.S. Huang , How Deep Neural Net-\n",
      "works can Improve Emotion Recognition on Video Data, IEEE Conf. Image Pro-\n",
      "cess (ICIP) (2016) .\n",
      "[32] V. Chernykh, G. Sterling, P. Prihodko, Emotion recognition from speech with\n",
      "recurrent neural networks, arXiv: 1701.08071v1 [cs.CL], 2017\n",
      "[33] Y. Fan , X. Lu , D. Li , Y. Liu , Video-based emotion recognition using CNN-RNN\n",
      "and C3D hybrid networks, in: ACM International Conference on Multimodal\n",
      "Interaction (ICMI 2016), 2016, pp. 445–450 .\n",
      "[34] S. Hochreiter , J. Schmidhuber , Long short-term memory, Neural Comput. 9 (8)\n",
      "(1997) 1735–1780 .\n",
      "[35] J.  Donahue,  L.A.  Hendricks,  S.  Guadarrama,  M.  Rohrbach,  S.  Venugopalan,\n",
      "K. Saenko, T. Darrell, Long-Term Recurrent Convolutional Networks for Visual\n",
      "Recognition and Description, IEEE Trans. Pattern Anal. Mach. Intell. 39 (4)\n",
      "(2017) 677–691, doi: 10.1109/TPAMI.2016.2599174 .\n",
      "[36] Q.V. Le, N. Jaitly, and G.E. Hinton. A simple way to initialize recurrent networks\n",
      "of rectiﬁed linear units. arXiv: 1504.00941 , 2015.\n",
      "[37] J. Susskind, A. Anderson, and G. Hinton. The toronto face database. Technical\n",
      "report, UTML TR 2010-001, University of Toronto, 2010.\n",
      "[38] M.J.  Lyons,  J.  Budynek,  S.  Akamatsu,  Automatic  classiﬁcation  of  single  fa-\n",
      "cial images, IEEE Trans. Pattern Anal. Mach. Intell. 21 (12) (1999) 1357–1362,\n",
      "doi: 10.1109/34.817413 .\n",
      "[39] A. Dhall , R. Goecke , J. Joshi , K. Sikka , T. Gedeon , Emotion recognition in the\n",
      "wild challenge 2014: baseline, data and protocol, in: International Conference\n",
      "on Multimodal Interaction, ICMI ’14, 2014, pp. 461–466 .\n",
      "[40] Y. Sun , X. Wang , X. Tang , Deep convolutional network cascade for facial point\n",
      "detection, in: IEEE Conference on Computer Vision and Pattern Recognition,\n",
      "CVPR ’13, 2013, pp. 3476–3483 .\n",
      "[41] I. Sutskever , J. Martens , G. Dahl , G. Hinton , On the importance of initialization\n",
      "and momentum in deep learning, in: Proceedings of the 30th International\n",
      "Conference on Machine Learning, 2013, pp. 1139–1147. D. P. Kingma and J. Ba.\n",
      "Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014 .\n",
      "[42] Xie S. , Hu H. , Facial expression recognition with FRR – CNN, Electron. Lett. 53\n",
      "(4) (2017) 235–237 .\n",
      "[43] S. Ouellet, Real-time emotion recognition for gaming using deep convolutional\n",
      "network features, CoRR, vol. abs/1408.3750, 2014.\n",
      "[44] G. Levi , T. Hassner , Emotion recognition in the wild via convolutional neural\n",
      "networks and mapped binary patterns, in: Proc. ACM International Conference\n",
      "on Multimodal Interaction (ICMI), November, 2015 .\n",
      "[45] D.K. Jain , R. Kumar , N. Jain , Decision-based spectral embedding approach for\n",
      "identifying facial behaviour on RGB-D images, Int. Conf. Commun. Netw. 508\n",
      "(2017) 677–687 .\n",
      "[46] D.K. Jain , Z. Zhang , K. Huang , Hybrid patch based diagonal pattern geometric\n",
      "appearance model for facial expression recognition, in: Conference on Intelli-\n",
      "gent Visual Surveillance, 2016, pp. 107–113 .\n",
      "[47] D.K. Jain, Z. Zhang, K. Huang, Multi angle optimal pattern-based deep learn-\n",
      "ing for automatic facial expression recognition, Pattern Recognit. Lett. (2017),\n",
      "doi: 10.1016/j.patrec.2017.06.025 .\n",
      "[48] D.K. Jain, Z. Zhang, K. Huang, Random walk-based feature learning for micro-\n",
      "expression recognition, 2018. https://doi.org/10.1016/j.patrec.2018.02.004 .\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar, LTTextLine\n",
    "\n",
    "pdf_file = 'testerpdf.pdf'\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    for page_layout in extract_pages(pdf_path):\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                for text_line in element:\n",
    "                    if isinstance(text_line, LTTextLine):\n",
    "                        print(text_line.get_text().strip())\n",
    "\n",
    "extract_text_from_pdf(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
